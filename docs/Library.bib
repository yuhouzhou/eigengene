
@article{galar_review_2012,
	title = {A {Review} on {Ensembles} for the {Class} {Imbalance} {Problem}: {Bagging}-, {Boosting}-, and {Hybrid}-{Based} {Approaches}},
	volume = {42},
	issn = {1094-6977, 1558-2442},
	shorttitle = {A {Review} on {Ensembles} for the {Class} {Imbalance} {Problem}},
	url = {http://ieeexplore.ieee.org/document/5978225/},
	doi = {10.1109/TSMCC.2011.2161285},
	abstract = {Classiﬁer learning with data-sets that suffer from imbalanced class distributions is a challenging problem in data mining community. This issue occurs when the number of examples that represent one class is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. In machine learning, the ensemble of classiﬁers are known to increase the accuracy of single classiﬁers by combining several of them, but neither of these learning techniques alone solve the class imbalance problem, to deal with this issue the ensemble learning algorithms have to be designed speciﬁcally. In this paper, our aim is to review the state of the art on ensemble techniques in the framework of imbalanced data-sets, with focus on two-class problems. We propose a taxonomy for ensemble-based methods to address the class imbalance where each proposal can be categorized depending on the inner ensemble methodology in which it is based. In addition, we develop a thorough empirical comparison by the consideration of the most signiﬁcant published approaches, within the families of the taxonomy proposed, to show whether any of them makes a difference. This comparison has shown the good behavior of the simplest approaches which combine random undersampling techniques with bagging or boosting ensembles. In addition, the positive synergy between sampling techniques and bagging has stood out. Furthermore, our results show empirically that ensemble-based algorithms are worthwhile since they outperform the mere use of preprocessing techniques before learning the classiﬁer, therefore justifying the increase of complexity by means of a signiﬁcant enhancement of the results.},
	language = {en},
	number = {4},
	urldate = {2018-11-28},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Galar, M. and Fernandez, A. and Barrenechea, E. and Bustince, H. and Herrera, F.},
	month = jul,
	year = {2012},
	pages = {463--484},
	file = {Galar et al. - 2012 - A Review on Ensembles for the Class Imbalance Prob.pdf:/Users/YuhouZhou/Zotero/storage/XZAVLIPX/Galar et al. - 2012 - A Review on Ensembles for the Class Imbalance Prob.pdf:application/pdf}
}

@incollection{goos_smoteboost:_2003,
	address = {Berlin, Heidelberg},
	title = {{SMOTEBoost}: {Improving} {Prediction} of the {Minority} {Class} in {Boosting}},
	volume = {2838},
	isbn = {978-3-540-20085-7 978-3-540-39804-2},
	shorttitle = {{SMOTEBoost}},
	url = {http://link.springer.com/10.1007/978-3-540-39804-2_12},
	abstract = {Many real world data mining applications involve learning from imbalanced data sets. Learning from data sets that contain very few instances of the minority (or interesting) class usually produces biased classifiers that have a higher predictive accuracy over the majority class(es), but poorer predictive accuracy over the minority class. SMOTE (Synthetic Minority Over-sampling TEchnique) is specifically designed for learning from imbalanced data sets. This paper presents a novel approach for learning from imbalanced data sets, based on a combination of the SMOTE algorithm and the boosting procedure. Unlike standard boosting where all misclassified examples are given equal weights, SMOTEBoost creates synthetic examples from the rare or minority class, thus indirectly changing the updating weights and compensating for skewed distributions. SMOTEBoost applied to several highly and moderately imbalanced data sets shows improvement in prediction performance on the minority class and overall improved F-values.},
	language = {en},
	urldate = {2018-11-28},
	booktitle = {Knowledge {Discovery} in {Databases}: {PKDD} 2003},
	publisher = {Springer Berlin Heidelberg},
	author = {Chawla, Nitesh V. and Lazarevic, Aleksandar and Hall, Lawrence O. and Bowyer, Kevin W.},
	editor = {Goos, Gerhard and Hartmanis, Juris and van Leeuwen, Jan and Lavrač, Nada and Gamberger, Dragan and Todorovski, Ljupčo and Blockeel, Hendrik},
	year = {2003},
	doi = {10.1007/978-3-540-39804-2_12},
	pages = {107--119},
	file = {Chawla et al. - 2003 - SMOTEBoost Improving Prediction of the Minority C.pdf:/Users/YuhouZhou/Zotero/storage/UJE897H6/Chawla et al. - 2003 - SMOTEBoost Improving Prediction of the Minority C.pdf:application/pdf}
}

@article{chawla_editorial:_2004,
	title = {Editorial: special issue on learning from imbalanced data sets},
	volume = {6},
	issn = {19310145},
	shorttitle = {Editorial},
	url = {http://portal.acm.org/citation.cfm?doid=1007730.1007733},
	doi = {10.1145/1007730.1007733},
	language = {en},
	number = {1},
	urldate = {2018-11-28},
	journal = {ACM SIGKDD Explorations Newsletter},
	author = {Chawla, Nitesh V. and Japkowicz, Nathalie and Kotcz, Aleksander},
	month = jun,
	year = {2004},
	pages = {1},
	file = {Chawla et al. - 2004 - Editorial special issue on learning from imbalanc.pdf:/Users/YuhouZhou/Zotero/storage/Z93UEYJ3/Chawla et al. - 2004 - Editorial special issue on learning from imbalanc.pdf:application/pdf}
}

@article{susuri_class_2017,
	title = {The {Class} {Imbalance} {Problem} in the {Machine} {Learning} {Based} {Detection} of {Vandalism} in {Wikipedia} across {Languages}},
	volume = {2},
	issn = {24156698},
	url = {http://astesj.com/v02/i01/p03/},
	doi = {10.25046/aj020103},
	abstract = {This paper analyses the impact of current trend in applying machine learning in detection of vandalism, with the specific aim of analyzing the impact of the class imbalance in Wikipedia articles. The class imbalance problem has the effect that almost all the examples are labelled as one class (legitimate editing); while far fewer examples are labelled as the other class, usually the more important class (vandalism). The obtained results show that resampling strategies: Random Under Sampling (RUS) and Synthetic Minority Oversampling TEchnique (SMOTE) have a partial effect on the improvement of the classification performance of all tested classifiers, excluding Random Forest, on both tested languages (simple English and Albanian) of the Wikipedia. The results from experimentation extended on two different languages show that they are comparable to the existing work.},
	language = {en},
	number = {1},
	urldate = {2018-12-01},
	journal = {Advances in Science, Technology and Engineering Systems Journal},
	author = {Susuri, Arsim and Hamiti, Mentor and Dika, Agni},
	month = jan,
	year = {2017},
	pages = {16--22},
	file = {Susuri et al. - 2017 - The Class Imbalance Problem in the Machine Learnin.pdf:/Users/YuhouZhou/Zotero/storage/ZR39XCF4/Susuri et al. - 2017 - The Class Imbalance Problem in the Machine Learnin.pdf:application/pdf}
}

@article{he_characteristic_nodate,
	title = {Characteristic {Subspace} {Learning} for {Time} {Series} {Classiﬁcation}},
	abstract = {This paper presents a novel time series classiﬁcation algorithm. It exploits time-delay embedding to transform time series into a set of points as a distribution, and attempt to classify time series by classifying corresponding distributions. It proposes a novel geometrical feature, i.e. characteristic subspace, from embedding points for classiﬁcation, and leverages classweighted support vector machine (SVM) to learn for it. An efﬁcient boosting strategy is also developed to enable a linear time training. The experiments show great potentials of this novel algorithm on accuracy, efﬁciency and interpretability.},
	language = {en},
	author = {He, Yuanduo and Pei, Jialiang and Chu, Xu and Wang, Yasha and Jin, Zhu and Peng, Guangju},
	pages = {7},
	file = {He et al. - Characteristic Subspace Learning for Time Series C.pdf:/Users/YuhouZhou/Zotero/storage/97XGU77S/He et al. - Characteristic Subspace Learning for Time Series C.pdf:application/pdf}
}

@article{chen_robust_2018,
	title = {Robust {Active} {Learning} for {Electrocardiographic} {Signal} {Classification}},
	url = {http://arxiv.org/abs/1811.08919},
	abstract = {The classiﬁcation of electrocardiographic (ECG) signals is a challenging problem for healthcare industry. Traditional supervised learning methods require a large number of labeled data which is usually expensive and difﬁcult to obtain for ECG signals. Active learning is well-suited for ECG signal classiﬁcation as it aims at selecting the best set of labeled data in order to maximize the classiﬁcation performance. Motivated by the fact that ECG data are usually heavily unbalanced among different classes and the class labels are noisy as they are manually labeled, this paper proposes a novel solution based on robust active learning for addressing these challenges. The key idea is to ﬁrst apply the clustering of the data in a low dimensional embedded space and then select the most information instances within local clusters. By selecting the most informative instances relying on local average minimal distances, the algorithm tends to select the data for labelling in a more diversiﬁed way. Finally, the robustness of the model is further enhanced by adding a novel noisy label reduction scheme after the selection of the labeled data. Experiments on the ECG signal classiﬁcation from the MIT-BIH arrhythmia database demonstrate the effectiveness of the proposed algorithm.},
	language = {en},
	urldate = {2018-12-01},
	journal = {arXiv:1811.08919 [cs, eess, stat]},
	author = {Chen, Xu and Sethi, Saratendu},
	month = nov,
	year = {2018},
	note = {arXiv: 1811.08919},
	keywords = {Computer Science - Machine Learning, Electrical Engineering and Systems Science - Signal Processing, Statistics - Machine Learning},
	annote = {Comment: Machine Learning for Health (ML4H) Workshop at NeurIPS 2018 arXiv:1811.07216 3},
	file = {Chen and Sethi - 2018 - Robust Active Learning for Electrocardiographic Si.pdf:/Users/YuhouZhou/Zotero/storage/TRUX3UHS/Chen and Sethi - 2018 - Robust Active Learning for Electrocardiographic Si.pdf:application/pdf}
}

@article{ming_lung_2018,
	title = {Lung {Disease} {Classification} using {GLCM} and {Deep} {Features} from {Different} {Deep} {Learning} {Architectures} with {Principal} {Component} {Analysis}},
	volume = {10},
	abstract = {Lung disease classification is an important stage in implementing a Computer Aided Diagnosis (CADx) system. CADx systems can aid doctors as a second rater to increase diagnostic accuracy for medical applications. It has also potential to reduce waiting time and increasing patient throughput when hospitals high workload. Conventional lung classification systems utilize textural features. However textural features may not be enough to describe properties of an image. Deep features are an emerging source of features that can combat the weaknesses of textural features. The goal of this study is to propose a lung disease classification framework using deep features from five different deep networks and comparing its results with the conventional Gray-level Co-occurrence Matrix (GLCM). This study used a dataset of 81 diseased and 15 normal patients with five levels of High Resolution Computed Tomography (HRCT) slices. A comparison of five different deep learning networks namely, Alexnet, VGG16, VGG19, Res50 and Res101, with textural features from Gray-level Co-occurrence Matrix (GLCM) was performed. This study used a K-fold validation protocol with K= 2, 3, 5 and 10. This study also compared using five classifiers; Decision Tree, Support Vector Machine, Linear Discriminant Analysis, Regression and k-nearest neighbor (k-NN) classifiers. The usage of PCA increased the classification accuracy from 92.01\% to 97.40\% when using k-NN classifier. This was achieved with only using 14 features instead of the initial 1000 features. Using SVM classifier, a maximum accuracy of 100\% was achieved when using all five of the deep learning features. Thus deep features show a promising application for classifying diseased and normal lungs.},
	language = {en},
	number = {7},
	author = {Ming, Joel Than Chia and Noor, Norliza Mohd and Rijal, Omar Mohd and Yunus, Ashari},
	year = {2018},
	pages = {14},
	file = {Ming et al. - 2018 - Lung Disease Classification using GLCM and Deep Fe.pdf:/Users/YuhouZhou/Zotero/storage/D2PVUS57/Ming et al. - 2018 - Lung Disease Classification using GLCM and Deep Fe.pdf:application/pdf}
}

@article{chawla_smote:_2002,
	title = {{SMOTE}: {Synthetic} {Minority} {Over}-sampling {Technique}},
	volume = {16},
	issn = {1076-9757},
	shorttitle = {{SMOTE}},
	url = {https://www.jair.org/index.php/jair/article/view/10302},
	doi = {10.1613/jair.953},
	language = {en-US},
	urldate = {2018-12-01},
	journal = {Journal of Artificial Intelligence Research},
	author = {Chawla, N. V. and Bowyer, K. W. and Hall, L. O. and Kegelmeyer, W. P.},
	month = jun,
	year = {2002},
	pages = {321--357},
	annote = {This essay proposed SMOTE},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/TGGXKW5Q/Chawla et al. - 2002 - SMOTE Synthetic Minority Over-sampling Technique.pdf:application/pdf;Snapshot:/Users/YuhouZhou/Zotero/storage/NQG4P32Q/10302.html:text/html}
}

@article{liu_exploratory_2009,
	title = {Exploratory {Undersampling} for {Class}-{Imbalance} {Learning}},
	volume = {39},
	issn = {1083-4419},
	doi = {10.1109/TSMCB.2008.2007853},
	abstract = {Undersampling is a popular method in dealing with class-imbalance problems, which uses only a subset of the majority class and thus is very efficient. The main deficiency is that many majority class examples are ignored. We propose two algorithms to overcome this deficiency. EasyEnsemble samples several subsets from the majority class, trains a learner using each of them, and combines the outputs of those learners. BalanceCascade trains the learners sequentially, where in each step, the majority class examples that are correctly classified by the current trained learners are removed from further consideration. Experimental results show that both methods have higher Area Under the ROC Curve, F-measure, and G-mean values than many existing class-imbalance learning methods. Moreover, they have approximately the same training time as that of undersampling when the same number of weak classifiers is used, which is significantly faster than other methods.},
	number = {2},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics)},
	author = {Liu, X. and Wu, J. and Zhou, Z.},
	month = apr,
	year = {2009},
	keywords = {BalanceCascade, class-imbalance learning, Class-imbalance learning, Costs, Credit cards, data mining, Data mining, EasyEnsemble, ensemble learning, Error analysis, F-measure, G-mean, Laboratories, learning (artificial intelligence), Learning systems, machine learning, Machine learning, Research and development, Sampling methods, undersampling},
	pages = {539--550},
	file = {IEEE Xplore Abstract Record:/Users/YuhouZhou/Zotero/storage/M46MYR2V/4717268.html:text/html;IEEE Xplore Full Text PDF:/Users/YuhouZhou/Zotero/storage/PBZ7XBBL/Liu et al. - 2009 - Exploratory Undersampling for Class-Imbalance Lear.pdf:application/pdf}
}

@inproceedings{han_borderline-smote:_2005,
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Borderline-{SMOTE}: {A} {New} {Over}-{Sampling} {Method} in {Imbalanced} {Data} {Sets} {Learning}},
	isbn = {978-3-540-31902-3},
	shorttitle = {Borderline-{SMOTE}},
	abstract = {In recent years, mining with imbalanced data sets receives more and more attentions in both theoretical and practical aspects. This paper introduces the importance of imbalanced data sets and their broad application domains in data mining, and then summarizes the evaluation metrics and the existing methods to evaluate and solve the imbalance problem. Synthetic minority over-sampling technique (SMOTE) is one of the over-sampling methods addressing this problem. Based on SMOTE method, this paper presents two new minority over-sampling methods, borderline-SMOTE1 and borderline-SMOTE2, in which only the minority examples near the borderline are over-sampled. For the minority class, experiments show that our approaches achieve better TP rate and F-value than SMOTE and random over-sampling methods.},
	language = {en},
	booktitle = {Advances in {Intelligent} {Computing}},
	publisher = {Springer Berlin Heidelberg},
	author = {Han, Hui and Wang, Wen-Yuan and Mao, Bing-Huan},
	editor = {Huang, De-Shuang and Zhang, Xiao-Ping and Huang, Guang-Bin},
	year = {2005},
	keywords = {Data Mining Algorithm, Imbalance Problem, Minimax Probability Machine, Minority Class, Traditional Data Mining},
	pages = {878--887},
	file = {Springer Full Text PDF:/Users/YuhouZhou/Zotero/storage/I6CL6LHH/Han et al. - 2005 - Borderline-SMOTE A New Over-Sampling Method in Im.pdf:application/pdf}
}

@article{sun_cost-sensitive_2007,
	title = {Cost-sensitive boosting for classification of imbalanced data},
	volume = {40},
	issn = {0031-3203},
	url = {http://www.sciencedirect.com/science/article/pii/S0031320307001835},
	doi = {10.1016/j.patcog.2007.04.009},
	abstract = {Classification of data with imbalanced class distribution has posed a significant drawback of the performance attainable by most standard classifier learning algorithms, which assume a relatively balanced class distribution and equal misclassification costs. The significant difficulty and frequent occurrence of the class imbalance problem indicate the need for extra research efforts. The objective of this paper is to investigate meta-techniques applicable to most classifier learning algorithms, with the aim to advance the classification of imbalanced data. The AdaBoost algorithm is reported as a successful meta-technique for improving classification accuracy. The insight gained from a comprehensive analysis of the AdaBoost algorithm in terms of its advantages and shortcomings in tacking the class imbalance problem leads to the exploration of three cost-sensitive boosting algorithms, which are developed by introducing cost items into the learning framework of AdaBoost. Further analysis shows that one of the proposed algorithms tallies with the stagewise additive modelling in statistics to minimize the cost exponential loss. These boosting algorithms are also studied with respect to their weighting strategies towards different types of samples, and their effectiveness in identifying rare cases through experiments on several real world medical data sets, where the class imbalance problem prevails.},
	number = {12},
	urldate = {2018-12-02},
	journal = {Pattern Recognition},
	author = {Sun, Yanmin and Kamel, Mohamed S. and Wong, Andrew K. C. and Wang, Yang},
	month = dec,
	year = {2007},
	keywords = {AdaBoost, Class imbalance problem, Classification, Cost-sensitive learning},
	pages = {3358--3378},
	file = {ScienceDirect Full Text PDF:/Users/YuhouZhou/Zotero/storage/GP3RJ4LL/Sun et al. - 2007 - Cost-sensitive boosting for classification of imba.pdf:application/pdf;ScienceDirect Snapshot:/Users/YuhouZhou/Zotero/storage/R5NKKLYW/S0031320307001835.html:text/html}
}

@article{galar_review_2012-1,
	title = {A {Review} on {Ensembles} for the {Class} {Imbalance} {Problem}: {Bagging}-, {Boosting}-, and {Hybrid}-{Based} {Approaches}},
	volume = {42},
	issn = {1094-6977},
	shorttitle = {A {Review} on {Ensembles} for the {Class} {Imbalance} {Problem}},
	doi = {10.1109/TSMCC.2011.2161285},
	abstract = {Classifier learning with data-sets that suffer from imbalanced class distributions is a challenging problem in data mining community. This issue occurs when the number of examples that represent one class is much lower than the ones of the other classes. Its presence in many real-world applications has brought along a growth of attention from researchers. In machine learning, the ensemble of classifiers are known to increase the accuracy of single classifiers by combining several of them, but neither of these learning techniques alone solve the class imbalance problem, to deal with this issue the ensemble learning algorithms have to be designed specifically. In this paper, our aim is to review the state of the art on ensemble techniques in the framework of imbalanced data-sets, with focus on two-class problems. We propose a taxonomy for ensemble-based methods to address the class imbalance where each proposal can be categorized depending on the inner ensemble methodology in which it is based. In addition, we develop a thorough empirical comparison by the consideration of the most significant published approaches, within the families of the taxonomy proposed, to show whether any of them makes a difference. This comparison has shown the good behavior of the simplest approaches which combine random undersampling techniques with bagging or boosting ensembles. In addition, the positive synergy between sampling techniques and bagging has stood out. Furthermore, our results show empirically that ensemble-based algorithms are worthwhile since they outperform the mere use of preprocessing techniques before learning the classifier, therefore justifying the increase of complexity by means of a significant enhancement of the results.},
	number = {4},
	journal = {IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews)},
	author = {Galar, M. and Fernandez, A. and Barrenechea, E. and Bustince, H. and Herrera, F.},
	month = jul,
	year = {2012},
	keywords = {data mining, learning (artificial intelligence), Learning systems, machine learning, Accuracy, Algorithm design and analysis, Bagging, bagging-based approach, boosting, boosting-based approach, class distribution, class imbalance problem, classification, classifier ensemble, classifier learning, data mining community, ensemble learning algorithms, ensemble-based algorithms, ensemble-based method taxonomy, ensembles, hybrid-based approach, imbalanced data-sets, inner ensemble methodology, multiple classifier systems, Noise, pattern classification, preprocessing techniques, Proposals, random undersampling techniques, Training, two-class problems},
	pages = {463--484},
	file = {IEEE Xplore Abstract Record:/Users/YuhouZhou/Zotero/storage/S6XQZWKI/5978225.html:text/html;IEEE Xplore Full Text PDF:/Users/YuhouZhou/Zotero/storage/SVVY9HIM/Galar et al. - 2012 - A Review on Ensembles for the Class Imbalance Prob.pdf:application/pdf}
}

@incollection{chawla_data_2010,
	address = {Boston, MA},
	title = {Data {Mining} for {Imbalanced} {Datasets}: {An} {Overview}},
	isbn = {978-0-387-09823-4},
	shorttitle = {Data {Mining} for {Imbalanced} {Datasets}},
	url = {https://doi.org/10.1007/978-0-387-09823-4_45},
	abstract = {SummaryA dataset is imbalanced if the classification categories are not approximately equally represented. Recent years brought increased interest in applying machine learning techniques to difficult “real-world” problems, many of which are characterized by imbalanced data. Additionally the distribution of the testing data may differ from that of the training data, and the true misclassification costs may be unknown at learning time. Predictive accuracy, a popular choice for evaluating performance of a classifier, might not be appropriate when the data is imbalanced and/or the costs of different errors vary markedly. In this Chapter, we discuss some of the sampling techniques used for balancing the datasets, and the performance measures more appropriate for mining imbalanced datasets.},
	language = {en},
	urldate = {2018-12-02},
	booktitle = {Data {Mining} and {Knowledge} {Discovery} {Handbook}},
	publisher = {Springer US},
	author = {Chawla, Nitesh V.},
	editor = {Maimon, Oded and Rokach, Lior},
	year = {2010},
	doi = {10.1007/978-0-387-09823-4_45},
	keywords = {classification, cost-sensitive measures, imbalanced datasets, precision and recall, ROC, sampling},
	pages = {875--886}
}

@book{seiffert_rusboost:_nodate,
	title = {{RUSBoost}: {A} {Hybrid} {Approach} to {Alleviating} {Class} {Imbalance}},
	shorttitle = {{RUSBoost}},
	abstract = {Abstract—Class imbalance is a problem that is common to many application domains. When examples of one class in a training data set vastly outnumber examples of the other class(es), traditional data mining algorithms tend to create suboptimal classification models. Several techniques have been used to alleviate the problem of class imbalance, including data sampling and boosting. In this paper, we present a new hybrid sampling/boosting algorithm, called RUSBoost, for learning from skewed training data. This algorithm provides a simpler and faster alternative to SMOTEBoost, which is another algorithm that combines boosting and data sampling. This paper evaluates the performances of RUSBoost and SMOTEBoost, as well as their individual components (random undersampling, synthetic minority oversampling technique, and AdaBoost). We conduct experiments using 15 data sets from various application domains, four base learners, and four evaluation metrics. RUSBoost and SMOTEBoost both outperform the other procedures, and RUSBoost performs comparably to (and often better than) SMOTEBoost while being a simpler and faster technique. Given these experimental results, we highly recommend RUSBoost as an attractive alternative for improving the classification performance of learners built using imbalanced data. Index Terms—Binary classification, boosting, class imbalance, RUSBoost, sampling. I.},
	author = {Seiffert, Chris and Khoshgoftaar, Taghi M. and Hulse, Jason Van and Napolitano, Amri},
	file = {Citeseer - Full Text PDF:/Users/YuhouZhou/Zotero/storage/MRN24IX5/Seiffert et al. - RUSBoost A Hybrid Approach to Alleviating Class I.pdf:application/pdf;Citeseer - Snapshot:/Users/YuhouZhou/Zotero/storage/8A36J5SZ/summary.html:text/html}
}

@article{longadge_class_2013,
	title = {Class {Imbalance} {Problem} in {Data} {Mining} {Review}},
	url = {http://arxiv.org/abs/1305.1707},
	abstract = {In last few years there are major changes and evolution has been done on classification of data. As the application area of technology is increases the size of data also increases. Classification of data becomes difficult because of unbounded size and imbalance nature of data. Class imbalance problem become greatest issue in data mining. Imbalance problem occur where one of the two classes having more sample than other classes. The most of algorithm are more focusing on classification of major sample while ignoring or misclassifying minority sample. The minority samples are those that rarely occur but very important. There are different methods available for classification of imbalance data set which is divided into three main categories, the algorithmic approach, data-preprocessing approach and feature selection approach. Each of this technique has their own advantages and disadvantages. In this paper systematic study of each approach is define which gives the right direction for research in class imbalance problem.},
	urldate = {2018-12-02},
	journal = {arXiv:1305.1707 [cs]},
	author = {Longadge, Rushi and Dongre, Snehalata},
	month = may,
	year = {2013},
	note = {arXiv: 1305.1707},
	keywords = {Computer Science - Machine Learning},
	file = {arXiv\:1305.1707 PDF:/Users/YuhouZhou/Zotero/storage/Y6P459BW/Longadge and Dongre - 2013 - Class Imbalance Problem in Data Mining Review.pdf:application/pdf;arXiv.org Snapshot:/Users/YuhouZhou/Zotero/storage/8WFXLRBD/1305.html:text/html}
}

@article{moniz_smoteboost_nodate,
	title = {{SMOTEBoost} for {Regression}: {Improving} the {Prediction} of {Extreme} {Values}},
	abstract = {Supervised learning with imbalanced domains is one of the biggest challenges in machine learning. Such tasks differ from standard learning tasks by assuming a skewed distribution of target variables, and user domain preference towards underrepresented cases. Most research has focused on imbalanced classiﬁcation tasks, where a wide range of solutions has been tested. Still, little work has been done concerning imbalanced regression tasks. In this paper, we propose an adaptation of the SMOTEBoost approach for the problem of imbalanced regression. Originally designed for classiﬁcation tasks, it combines boosting methods and the SMOTE resampling strategy. We present four variants of SMOTEBoost and provide an experimental evaluation using 30 datasets with an extensive analysis of results in order to assess the ability of SMOTEBoost methods in predicting extreme target values, and their predictive trade-off concerning baseline boosting methods. SMOTEBoost is publicly available in a software package.},
	language = {en},
	author = {Moniz, Nuno and Ribeiro, Rita P and Cerqueira, Vitor and Chawla, Nitesh},
	pages = {11},
	file = {Moniz et al. - SMOTEBoost for Regression Improving the Predictio.pdf:/Users/YuhouZhou/Zotero/storage/PWQTWFWG/Moniz et al. - SMOTEBoost for Regression Improving the Predictio.pdf:application/pdf}
}

@article{moniz_smoteboost_nodate-1,
	title = {{SMOTEBoost} for {Regression}: {Improving} the {Prediction} of {Extreme} {Values}},
	abstract = {Supervised learning with imbalanced domains is one of the biggest challenges in machine learning. Such tasks differ from standard learning tasks by assuming a skewed distribution of target variables, and user domain preference towards underrepresented cases. Most research has focused on imbalanced classiﬁcation tasks, where a wide range of solutions has been tested. Still, little work has been done concerning imbalanced regression tasks. In this paper, we propose an adaptation of the SMOTEBoost approach for the problem of imbalanced regression. Originally designed for classiﬁcation tasks, it combines boosting methods and the SMOTE resampling strategy. We present four variants of SMOTEBoost and provide an experimental evaluation using 30 datasets with an extensive analysis of results in order to assess the ability of SMOTEBoost methods in predicting extreme target values, and their predictive trade-off concerning baseline boosting methods. SMOTEBoost is publicly available in a software package.},
	language = {en},
	author = {Moniz, Nuno and Ribeiro, Rita P and Cerqueira, Vitor and Chawla, Nitesh},
	pages = {11},
	file = {Moniz et al. - SMOTEBoost for Regression Improving the Predictio.pdf:/Users/YuhouZhou/Zotero/storage/ICRKRXK7/Moniz et al. - SMOTEBoost for Regression Improving the Predictio.pdf:application/pdf}
}

@article{rayhan_cusboost:_2017,
	title = {{CUSBoost}: {Cluster}-based {Under}-sampling with {Boosting} for {Imbalanced} {Classification}},
	shorttitle = {{CUSBoost}},
	url = {http://arxiv.org/abs/1712.04356},
	doi = {10.1109/CSITSS.2017.8447534},
	abstract = {Class imbalance classification is a challenging research problem in data mining and machine learning, as most of the real-life datasets are often imbalanced in nature. Existing learning algorithms maximise the classification accuracy by correctly classifying the majority class, but misclassify the minority class. However, the minority class instances are representing the concept with greater interest than the majority class instances in real-life applications. Recently, several techniques based on sampling methods (under-sampling of the majority class and over-sampling the minority class), cost-sensitive learning methods, and ensemble learning have been used in the literature for classifying imbalanced datasets. In this paper, we introduce a new clustering-based under-sampling approach with boosting (AdaBoost) algorithm, called CUSBoost, for effective imbalanced classification. The proposed algorithm provides an alternative to RUSBoost (random under-sampling with AdaBoost) and SMOTEBoost (synthetic minority over-sampling with AdaBoost) algorithms. We evaluated the performance of CUSBoost algorithm with the state-of-the-art methods based on ensemble learning like AdaBoost, RUSBoost, SMOTEBoost on 13 imbalance binary and multi-class datasets with various imbalance ratios. The experimental results show that the CUSBoost is a promising and effective approach for dealing with highly imbalanced datasets.},
	urldate = {2018-12-02},
	journal = {2017 2nd International Conference on Computational Systems and Information Technology for Sustainable Solution (CSITSS)},
	author = {Rayhan, Farshid and Ahmed, Sajid and Mahbub, Asif and Jani, Md Rafsan and Shatabda, Swakkhar and Farid, Dewan Md},
	month = dec,
	year = {2017},
	note = {arXiv: 1712.04356},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	pages = {1--5},
	annote = {Comment: CSITSS-2017},
	file = {arXiv\:1712.04356 PDF:/Users/YuhouZhou/Zotero/storage/77LN76EW/Rayhan et al. - 2017 - CUSBoost Cluster-based Under-sampling with Boosti.pdf:application/pdf;arXiv.org Snapshot:/Users/YuhouZhou/Zotero/storage/84DCMY7Q/1712.html:text/html}
}

@article{lin_clustering-based_2017,
	title = {Clustering-based undersampling in class-imbalanced data},
	volume = {409-410},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025517307235},
	doi = {10.1016/j.ins.2017.05.008},
	abstract = {Class imbalance is often a problem in various real-world data sets, where one class (i.e. the minority class) contains a small number of data points and the other (i.e. the majority class) contains a large number of data points. It is notably difficult to develop an effective model using current data mining and machine learning algorithms without considering data preprocessing to balance the imbalanced data sets. Random undersampling and oversampling have been used in numerous studies to ensure that the different classes contain the same number of data points. A classifier ensemble (i.e. a structure containing several classifiers) can be trained on several different balanced data sets for later classification purposes. In this paper, we introduce two undersampling strategies in which a clustering technique is used during the data preprocessing step. Specifically, the number of clusters in the majority class is set to be equal to the number of data points in the minority class. The first strategy uses the cluster centers to represent the majority class, whereas the second strategy uses the nearest neighbors of the cluster centers. A further study was conducted to examine the effect on performance of the addition or deletion of 5 to 10 cluster centers in the majority class. The experimental results obtained using 44 small-scale and 2 large-scale data sets revealed that the clustering-based undersampling approach with the second strategy outperformed five state-of-the-art approaches. Specifically, this approach combined with a single multilayer perceptron classifier and C4.5 decision tree classifier ensembles delivered optimal performance over both small- and large-scale data sets.},
	urldate = {2018-12-02},
	journal = {Information Sciences},
	author = {Lin, Wei-Chao and Tsai, Chih-Fong and Hu, Ya-Han and Jhang, Jing-Shang},
	month = oct,
	year = {2017},
	keywords = {Machine learning, Class imbalance, Classifier ensembles, Clustering, Imbalanced data},
	pages = {17--26},
	file = {ScienceDirect Full Text PDF:/Users/YuhouZhou/Zotero/storage/GMCZ4QY3/Lin et al. - 2017 - Clustering-based undersampling in class-imbalanced.pdf:application/pdf;ScienceDirect Snapshot:/Users/YuhouZhou/Zotero/storage/JRTRJR32/S0020025517307235.html:text/html}
}

@article{brandt_evaluating_2014,
	title = {Evaluating forecasts of political conflict dynamics},
	volume = {30},
	issn = {0169-2070},
	url = {http://www.sciencedirect.com/science/article/pii/S0169207014000612},
	doi = {10.1016/j.ijforecast.2014.03.014},
	abstract = {There is considerable interest today in the forecasting of conflict dynamics. Commonly, the root mean square error and other point metrics are used to evaluate the forecasts from such models. However, conflict processes are non-linear, so these point metrics often do not produce adequate evaluations of the calibration and sharpness of the forecast models. Forecast density evaluation improves the model evaluation. We review tools for density evaluation, including continuous rank probability scores, verification rank histograms, and sharpness plots. The usefulness of these tools for evaluating conflict forecasting models is explained. We illustrate this, first, in a comparison of several time series models’ forecasts of simulated data from a Markov-switching process, and second, in a comparison of several models’ abilities to forecast conflict dynamics in the Cross Straits. These applications show the pitfalls of relying on point metrics alone for evaluating the quality of conflict forecasting models. As in other fields, it is more useful to employ a suite of tools. A non-linear vector autoregressive model emerges as the model which is best able to forecast conflict dynamics between China and Taiwan.},
	number = {4},
	urldate = {2018-12-02},
	journal = {International Journal of Forecasting},
	author = {Brandt, Patrick T. and Freeman, John R. and Schrodt, Philip A.},
	month = oct,
	year = {2014},
	keywords = {Bayesian, Conflict dynamics, Density evaluation, Scoring rules, Time series, Verification rank histogram},
	pages = {944--962},
	file = {ScienceDirect Full Text PDF:/Users/YuhouZhou/Zotero/storage/2RGSVZWC/Brandt et al. - 2014 - Evaluating forecasts of political conflict dynamic.pdf:application/pdf;ScienceDirect Snapshot:/Users/YuhouZhou/Zotero/storage/4LY7J8Q2/S0169207014000612.html:text/html}
}

@article{clayton_will_2014,
	title = {Will we see helping hands? {Predicting} civil war mediation and likely success},
	volume = {31},
	issn = {0738-8942},
	shorttitle = {Will we see helping hands?},
	url = {https://doi.org/10.1177/0738894213508693},
	doi = {10.1177/0738894213508693},
	abstract = {We examine whether features highlighted as important for mediation in existing research allow us to predict when we will see mediation and likely success out-of-sample. We assess to what extent information about the characteristics of the conflicting dyads and conflict history can be evaluated ex ante and improve our ability to predict when conflicts will see mediation as well as when peaceful solutions are more likely to follow from mediation. We justify that the information used to generate predictions through the model can be assessed ex ante, using the ongoing conflict in Syria as an example. Our results suggest that a two-stage model of mediation and success seems to do relatively well overall in predicting when mediation is likely to occur, but notably less well in predicting the outcome of mediation. This may reflect how ex ante observable structural characteristics are likely to influence willingness to mediate, while the outcome of mediation to a large extent will be influenced by unobservable characteristics or private information and how these are influenced by mediation. We discuss the usefulness of out-of-sample evaluation in studying conflict management and suggest future directions for improving our ability to forecast mediation.},
	language = {en},
	number = {3},
	urldate = {2018-12-02},
	journal = {Conflict Management and Peace Science},
	author = {Clayton, Govinda and Gleditsch, Kristian Skrede},
	month = jul,
	year = {2014},
	pages = {265--284},
	file = {SAGE PDF Full Text:/Users/YuhouZhou/Zotero/storage/YR3MQXTN/Clayton and Gleditsch - 2014 - Will we see helping hands Predicting civil war me.pdf:application/pdf}
}

@article{elkan_foundations_nodate,
	title = {The {Foundations} of {Cost}-{Sensitive} {Learning}},
	abstract = {This paper revisits the problem of optimal learning and decision-making when different misclassiﬁcation errors incur different penalties. We characterize precisely but intuitively when a cost matrix is reasonable, and we show how to avoid the mistake of deﬁning a cost matrix that is economically incoherent. For the two-class case, we prove a theorem that shows how to change the proportion of negative examples in a training set in order to make optimal cost-sensitive classiﬁcation decisions using a classiﬁer learned by a standard non-costsensitive learning method. However, we then argue that changing the balance of negative and positive training examples has little effect on the classiﬁers produced by standard Bayesian and decision tree learning methods. Accordingly, the recommended way of applying one of these methods in a domain with differing misclassiﬁcation costs is to learn a classiﬁer from the training set as given, and then to compute optimal decisions explicitly using the probability estimates given by the classiﬁer.},
	language = {en},
	author = {Elkan, Charles},
	pages = {7},
	file = {Elkan - The Foundations of Cost-Sensitive Learning.pdf:/Users/YuhouZhou/Zotero/storage/CAF92352/Elkan - The Foundations of Cost-Sensitive Learning.pdf:application/pdf}
}

@inproceedings{liu_influence_2006,
	title = {The {Influence} of {Class} {Imbalance} on {Cost}-{Sensitive} {Learning}: {An} {Empirical} {Study}},
	isbn = {978-0-7695-2701-7},
	shorttitle = {The {Influence} of {Class} {Imbalance} on {Cost}-{Sensitive} {Learning}},
	url = {doi.ieeecomputersociety.org/10.1109/ICDM.2006.158},
	doi = {10.1109/ICDM.2006.158},
	abstract = {In real-world applications the number of examples in  one class may overwhelm the other class, but the primary  interest is usually on the minor class. Cost-sensitive  learning has been deeded as a good solution to these  class-imbalanced tasks, yet it is not clear how does the  class-imbalance affect cost-sensitive classifiers. This paper  presents an empirical study using 38 data sets, which discloses  that class-imbalance often affects the performance of  cost-sensitive classifiers: When the misclassification costs  are not seriously unequal, cost-sensitive classifiers generally  favor natural class distribution although it might be imbalanced;  while when misclassification costs are seriously  unequal, a balanced class distribution is more favorable.},
	urldate = {2018-12-02},
	booktitle = {Sixth {International} {Conference} on {Data} {Mining} ({ICDM}'06)({ICDM})},
	author = {Liu, X. and Zhou, Z.},
	year = {2006},
	keywords = {null},
	pages = {970--974},
	file = {Snapshot:/Users/YuhouZhou/Zotero/storage/ANIXZT23/270100970-abs.html:text/html;Submitted Version:/Users/YuhouZhou/Zotero/storage/VK49E67I/Liu and Zhou - 2006 - The Influence of Class Imbalance on Cost-Sensitive.pdf:application/pdf}
}

@article{weiss_learning_2003,
	title = {Learning {When} {Training} {Data} are {Costly}: {The} {Effect} of {Class} {Distribution} on {Tree} {Induction}},
	volume = {19},
	issn = {1076-9757},
	shorttitle = {Learning {When} {Training} {Data} are {Costly}},
	url = {https://www.jair.org/index.php/jair/article/view/10346},
	doi = {10.1613/jair.1199},
	language = {en-US},
	urldate = {2018-12-02},
	journal = {Journal of Artificial Intelligence Research},
	author = {Weiss, G. M. and Provost, F.},
	month = oct,
	year = {2003},
	pages = {315--354},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/DL2UAPP4/Weiss and Provost - 2003 - Learning When Training Data are Costly The Effect.pdf:application/pdf;Snapshot:/Users/YuhouZhou/Zotero/storage/KWQZCXM7/10346.html:text/html}
}

@inproceedings{seiffert_rusboost:_2008,
	title = {{RUSBoost}: {Improving} classification performance when training data is skewed},
	shorttitle = {{RUSBoost}},
	doi = {10.1109/ICPR.2008.4761297},
	abstract = {Constructing classification models using skewed training data can be a challenging task. We present RUSBoost, a new algorithm for alleviating the problem of class imbalance. RUSBoost combines data sampling and boosting, providing a simple and efficient method for improving classification performance when training data is imbalanced. In addition to performing favorably when compared to SMOTEBoost (another hybrid sampling/boosting algorithm), RUSBoost is computationally less expensive than SMOTEBoost and results in significantly shorter model training times. This combination of simplicity, speed and performance makes RUSBoost an excellent technique for learning from imbalanced data.},
	booktitle = {2008 19th {International} {Conference} on {Pattern} {Recognition}},
	author = {Seiffert, C. and Khoshgoftaar, T. M. and Hulse, J. Van and Napolitano, A.},
	month = dec,
	year = {2008},
	keywords = {Costs, data mining, Data mining, learning (artificial intelligence), machine learning, Sampling methods, Algorithm design and analysis, pattern classification, Boosting, boosting algorithm, class imbalance, classification model, data sampling, Diseases, Iterative algorithms, skewed training data, Training data, Voting},
	pages = {1--4},
	file = {IEEE Xplore Abstract Record:/Users/YuhouZhou/Zotero/storage/RN5CQI6U/4761297.html:text/html;IEEE Xplore Full Text PDF:/Users/YuhouZhou/Zotero/storage/VZPT33SR/Seiffert et al. - 2008 - RUSBoost Improving classification performance whe.pdf:application/pdf}
}

@inproceedings{zhu_ifme:_2013,
	address = {New York, NY, USA},
	series = {{JCDL} '13},
	title = {{IFME}: {Information} {Filtering} by {Multiple} {Examples} with {Under}-sampling in a {Digital} {Library} {Environment}},
	isbn = {978-1-4503-2077-1},
	shorttitle = {{IFME}},
	url = {http://doi.acm.org/10.1145/2467696.2467736},
	doi = {10.1145/2467696.2467736},
	abstract = {With the amount of digitalized documents increasing exponentially, it is more difficult for users to keep up to date with the knowledge in their domain. In this paper, we present a framework named IFME (Information Filtering by Multiple Examples) in a digital library environment to help users identify the literature related to their interests by leveraging the Positive Unlabeled learning (PU learning). Using a few relevant documents provided by a user and considering the documents in an online database as unlabeled data (called U), it ranks the documents in U using a PU learning algorithm. From the experimental results, we found that while the approach performed well when a large set of relevant feedback documents were available, it performed relatively poor when the relevant feedback documents were few. We improved IFME by combining PU learning with under-sampling to tune the performance. Using Mean Average Precision (MAP), our experimental results indicated that with under-sampling, the performance improved significantly even when the size of P was small. We believe the PU learning based IFME framework brings insights to develop more effective digital library systems.},
	urldate = {2018-12-04},
	booktitle = {Proceedings of the 13th {ACM}/{IEEE}-{CS} {Joint} {Conference} on {Digital} {Libraries}},
	publisher = {ACM},
	author = {Zhu, Mingzhu and Xu, Chao and Wu, Yi-Fang Brook},
	year = {2013},
	keywords = {information retrieval, positive unlabeled learning, relevance feedback, search by multiple examples, text classification},
	pages = {107--110},
	file = {ACM Full Text PDF:/Users/YuhouZhou/Zotero/storage/C2QYS373/Zhu et al. - 2013 - IFME Information Filtering by Multiple Examples w.pdf:application/pdf}
}

@article{elrahman_review_nodate,
	title = {A {Review} of {Class} {Imbalance} {Problem}},
	abstract = {Class imbalance is one of the challenges of machine to falsely detect them and the decision boundary be learning and data mining fields. Imbalance data sets degrades the performance of data mining and machine learning techniques as the overall accuracy and decision making be biased to the majority class, which lead to misclassifying the minority class samples or furthermore treated them as noise. This paper proposes a general survey for class imbalance problem solutions and the most significant investigations recently introduced by researchers.},
	language = {en},
	author = {Elrahman, Shaza M Abd and Abraham, Ajith},
	pages = {9},
	file = {Elrahman and Abraham - A Review of Class Imbalance Problem.pdf:/Users/YuhouZhou/Zotero/storage/42C8296D/Elrahman and Abraham - A Review of Class Imbalance Problem.pdf:application/pdf}
}

@article{zhu_ifme:_2013-1,
	title = {{IFME}: information filtering by multiple examples with under-sampling in a digital library environment},
	shorttitle = {{IFME}},
	url = {http://dl.acm.org/citation.cfm?id=2467696.2467736},
	doi = {10.1145/2467696.2467736},
	author = {Zhu, Mingzhu and Xu, Chao and Wu, Yi-Fang Brook},
	month = jul,
	year = {2013},
	pages = {107--110},
	file = {Oversampling and undersampling in data analysis - Wikiwand:/Users/YuhouZhou/Zotero/storage/DNSQSUHH/Oversampling_and_undersampling_in_data_analysis.html:text/html}
}

@misc{noauthor_phonetics_nodate,
	title = {phonetics - {Does} {Mandarin} {Chinese} have phonetically voiced plosives, fricatives, or affricates (besides "r" = [ʐ] / [ɻ])?},
	url = {https://linguistics.stackexchange.com/questions/4860/does-mandarin-chinese-have-phonetically-voiced-plosives-fricatives-or-affricat},
	urldate = {2019-01-02},
	journal = {Linguistics Stack Exchange},
	file = {Snapshot:/Users/YuhouZhou/Zotero/storage/VTL3PBLM/does-mandarin-chinese-have-phonetically-voiced-plosives-fricatives-or-affricat.html:text/html}
}

@article{scardapane_learning_2015,
	series = {{INNS} {Conference} on {Big} {Data} 2015 {Program} {San} {Francisco}, {CA}, {USA} 8-10 {August} 2015},
	title = {Learning from {Distributed} {Data} {Sources} {Using} {Random} {Vector} {Functional}-{Link} {Networks}},
	volume = {53},
	issn = {1877-0509},
	url = {http://www.sciencedirect.com/science/article/pii/S187705091501827X},
	doi = {10.1016/j.procs.2015.07.324},
	abstract = {One of the main characteristics in many real-world big data scenarios is their distributed nature. In a machine learning context, distributed data, together with the requirements of preserving privacy and scaling up to large networks, brings the challenge of designing fully decentralized training protocols. In this paper, we explore the problem of distributed learning when the features of every pattern are available throughout multiple agents (as is happening, for example, in a distributed database scenario). We propose an algorithm for a particular class of neural networks, known as Random Vector Functional-Link (RVFL), which is based on the Alternating Direction Method of Multipliers optimization algorithm. The proposed algorithm allows to learn an RVFL network from multiple distributed data sources, while restricting communication to the unique operation of computing a distributed average. Our experimental simulations show that the algorithm is able to achieve a generalization accuracy comparable to a fully centralized solution, while at the same time being extremely efficient.},
	urldate = {2019-01-17},
	journal = {Procedia Computer Science},
	author = {Scardapane, Simone and Panella, Massimo and Comminiello, Danilo and Uncini, Aurelio},
	month = jan,
	year = {2015},
	keywords = {Alternating Direction Method of Multipliers, Distributed learning, Multiple data sources, Random Vector Functional-Link},
	pages = {468--477},
	file = {ScienceDirect Full Text PDF:/Users/YuhouZhou/Zotero/storage/9L4SXA9U/Scardapane et al. - 2015 - Learning from Distributed Data Sources Using Rando.pdf:application/pdf;ScienceDirect Snapshot:/Users/YuhouZhou/Zotero/storage/5PUIZC98/S187705091501827X.html:text/html}
}

@article{pao_learning_1994,
	series = {Backpropagation, {Part} {IV}},
	title = {Learning and generalization characteristics of the random vector functional-link net},
	volume = {6},
	issn = {0925-2312},
	url = {http://www.sciencedirect.com/science/article/pii/0925231294900531},
	doi = {10.1016/0925-2312(94)90053-1},
	abstract = {In this paper we explore and discuss the learning and generalization characteristics of the random vector version of the Functional-link net and compare these with those attainable with the GDR algorithm. This is done for a well-behaved deterministic function and for real-world data. It seems that ‘overtraining’ occurs for stochastic mappings. Otherwise there is saturation of training.},
	number = {2},
	urldate = {2019-01-17},
	journal = {Neurocomputing},
	author = {Pao, Yoh-Han and Park, Gwang-Hoon and Sobajic, Dejan J.},
	month = apr,
	year = {1994},
	keywords = {auto-enhancement, functional mapping, Functional-link net, generalized delta rule, Neural net, overtraining and generalization},
	pages = {163--180},
	file = {ScienceDirect Snapshot:/Users/YuhouZhou/Zotero/storage/5FWFUJAS/0925231294900531.html:text/html}
}

@article{zhang_comprehensive_2016,
	title = {A comprehensive evaluation of random vector functional link networks},
	volume = {367-368},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025515006799},
	doi = {10.1016/j.ins.2015.09.025},
	abstract = {With randomly generated weights between input and hidden layers, a random vector functional link network is a universal approximator for continuous functions on compact sets with fast learning property. Though it was proposed two decades ago, the classification ability of this family of networks has not been fully investigated yet. Through a very comprehensive evaluation by using 121 UCI datasets, the effect of bias in the output layer, direct links from the input layer to the output layer and type of activation functions in the hidden layer, scaling of parameter randomization as well as the solution procedure for the output weights are investigated in this work. Surprisingly, we found that the direct link plays an important performance enhancing role in RVFL, while the bias term in the output neuron had no significant effect. The ridge regression based closed-form solution was better than those with Moore–Penrose pseudoinverse. Instead of using a uniform randomization in [−1,+1] for all datasets, tuning the scaling of the uniform randomization range for each dataset enhances the overall performance. Six commonly used activation functions were investigated in this work and we found that hardlim and sign activation functions degenerate the overall performance. These basic conclusions can serve as general guidelines for designing RVFL networks based classifiers.},
	urldate = {2019-01-17},
	journal = {Information Sciences},
	author = {Zhang, Le and Suganthan, P. N.},
	month = nov,
	year = {2016},
	keywords = {Data classification, Moore–Penrose pseudoinverse, Random vector functional link networks, Ridge regression},
	pages = {1094--1105},
	file = {ScienceDirect Full Text PDF:/Users/YuhouZhou/Zotero/storage/VYMHEPU2/Zhang and Suganthan - 2016 - A comprehensive evaluation of random vector functi.pdf:application/pdf;ScienceDirect Snapshot:/Users/YuhouZhou/Zotero/storage/9S9M5WKX/S0020025515006799.html:text/html}
}

@article{pao_functional-link_1992,
	title = {Functional-link net computing: theory, system architecture, and functionalities},
	volume = {25},
	issn = {0018-9162},
	shorttitle = {Functional-link net computing},
	doi = {10.1109/2.144401},
	abstract = {A system architecture and a network computational approach compatible with the goal of devising a general-purpose artificial neural network computer are described. The functionalities of supervised learning and optimization are illustrated, and cluster analysis and associative recall are briefly mentioned.{\textless}{\textless}ETX{\textgreater}{\textgreater}},
	number = {5},
	journal = {Computer},
	author = {Pao, Y.- and Takefuji, Y.},
	month = may,
	year = {1992},
	keywords = {Sampling methods, associative recall, cluster analysis, Clustering algorithms, Computer architecture, Computer errors, Computer networks, Equations, functional-link net computing, Gaussian approximation, Gaussian processes, general-purpose artificial neural network computer, learning systems, network computational approach, neural nets, optimisation, supervised learning, Supervised learning, system architecture},
	pages = {76--79},
	file = {IEEE Xplore Abstract Record:/Users/YuhouZhou/Zotero/storage/I8BMNT5C/144401.html:text/html;IEEE Xplore Full Text PDF:/Users/YuhouZhou/Zotero/storage/9K57WZZ4/Pao and Takefuji - 1992 - Functional-link net computing theory, system arch.pdf:application/pdf}
}

@article{ren_random_2016,
	title = {Random vector functional link network for short-term electricity load demand forecasting},
	volume = {367-368},
	issn = {0020-0255},
	url = {http://www.sciencedirect.com/science/article/pii/S0020025515008774},
	doi = {10.1016/j.ins.2015.11.039},
	abstract = {Short-term electricity load forecasting plays an important role in the energy market as accurate forecasting is beneficial for power dispatching, unit commitment, fuel allocation and so on. This paper reviews a few single hidden layer network configurations with random weights (RWSLFN). The RWSLFN was extended to eight variants based on the presence or absence of input layer bias, hidden layer bias and direct input–output connections. In order to avoid mapping the weighted inputs into the saturation region of the enhancement nodes’ activation function and to suppress the outliers in the input data, a quantile scaling algorithm to re-distribute the randomly weighted inputs is proposed. The eight variations of RWSLFN are assessed using six generic time series datasets and 12 load demand time series datasets. The result shows that the RWSLFNs with direct input–output connections (known as the random vector functional link network or RVFL network) have statistically significantly better performance than the RWSLFN configurations without direct input–output connections, possibly due to the fact that the direct input–output connections in the RVFL network emulate the time delayed finite impulse response (FIR) filter. However the RVFL network has simpler training and higher accuracy than the FIR based two stage neural network. The RVFL network is also compared with some reported forecasting methods. The RVFL network overall outperforms the non-ensemble methods, namely the persistence method, seasonal autoregressive integrated moving average (sARIMA), artificial neural network (ANN). In addition, the testing time of the RVFL network is the shortest while the training time is comparable to the other reported methods. Finally, possible future research directions are pointed out.},
	urldate = {2019-01-17},
	journal = {Information Sciences},
	author = {Ren, Ye and Suganthan, P. N. and Srikanth, N. and Amaratunga, Gehan},
	month = nov,
	year = {2016},
	keywords = {Electricity load demand forecasting, Neural network, Random vector functional link, Random weights, Time series forecasting},
	pages = {1078--1093},
	file = {ScienceDirect Full Text PDF:/Users/YuhouZhou/Zotero/storage/BVDAFXZT/Ren et al. - 2016 - Random vector functional link network for short-te.pdf:application/pdf;ScienceDirect Snapshot:/Users/YuhouZhou/Zotero/storage/2N5IMP4K/S0020025515008774.html:text/html}
}

@misc{noauthor_what_2018,
	title = {What is {Data} {Engineering}?},
	url = {https://www.datacamp.com/community/blog/data-engineering},
	abstract = {In this blog, you will learn what data engineering entails along with learning about our future data engineering course offerings.},
	urldate = {2019-01-22},
	journal = {DataCamp Community},
	month = sep,
	year = {2018},
	file = {Snapshot:/Users/YuhouZhou/Zotero/storage/J9MZ23BP/data-engineering.html:text/html}
}

@article{jaeger_using_2017,
	title = {Using {Conceptors} to {Manage} {Neural} {Long}-{Term} {Memories} for {Temporal} {Patterns}},
	volume = {18},
	url = {http://jmlr.org/papers/v18/15-449.html},
	number = {13},
	urldate = {2019-01-23},
	journal = {Journal of Machine Learning Research},
	author = {Jaeger, Herbert},
	year = {2017},
	pages = {1--43},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/5VE6WAFA/Jaeger - 2017 - Using Conceptors to Manage Neural Long-Term Memori.pdf:application/pdf;Snapshot:/Users/YuhouZhou/Zotero/storage/P4NRYK8H/15-449.html:text/html}
}

@article{jaeger_using_2017-1,
	title = {Using {Conceptors} to {Manage} {Neural} {Long}-{Term} {Memories} for {Temporal} {Patterns}},
	volume = {18},
	url = {http://jmlr.org/papers/v18/15-449.html},
	number = {13},
	urldate = {2019-01-24},
	journal = {Journal of Machine Learning Research},
	author = {Jaeger, Herbert},
	year = {2017},
	pages = {1--43},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/N5QUBWJM/Jaeger - 2017 - Using Conceptors to Manage Neural Long-Term Memori.pdf:application/pdf;Snapshot:/Users/YuhouZhou/Zotero/storage/T2LD7ED3/15-449.html:text/html}
}

@article{oconnor_ancient_nodate,
	title = {Ancient avian aria from {Antarctica}},
	language = {en},
	author = {O’Connor, Patrick M},
	pages = {2},
	file = {O’Connor - Ancient avian aria from Antarctica.pdf:/Users/YuhouZhou/Zotero/storage/J3K8Q89H/O’Connor - Ancient avian aria from Antarctica.pdf:application/pdf}
}

@article{franz_standard_2012,
	title = {Standard errors and confidence intervals in within-subjects designs: {Generalizing} {Loftus} and {Masson} (1994) and avoiding the biases of alternative accounts},
	volume = {19},
	issn = {1069-9384, 1531-5320},
	shorttitle = {Standard errors and confidence intervals in within-subjects designs},
	url = {http://www.springerlink.com/index/10.3758/s13423-012-0230-1},
	doi = {10.3758/s13423-012-0230-1},
	abstract = {Repeated measures designs are common in experimental psychology. Because of the correlational structure in these designs, the calculation and interpretation of confidence intervals is nontrivial. One solution was provided by Loftus and Masson (Psychonomic Bulletin \& Review 1:476–490, 1994). This solution, although widely adopted, has the limitation of implying same-size confidence intervals for all factor levels, and therefore does not allow for the assessment of variance homogeneity assumptions (i.e., the circularity assumption, which is crucial for the repeated measures ANOVA). This limitation and the method’s perceived complexity have sometimes led scientists to use a simplified variant, based on a per-subject normalization of the data (Bakeman \& McArthur, Behavior Research Methods, Instruments, \& Computers 28:584–589, 1996; Cousineau, Tutorials in Quantitative Methods for Psychology 1:42–45, 2005; Morey, Tutorials in Quantitative Methods for Psychology 4:61–64, 2008; Morrison \& Weaver, Behavior Research Methods, Instruments, \& Computers 27:52–56, 1995). We show that this normalization method leads to biased results and is uninformative with regard to circularity. Instead, we provide a simple, intuitive generalization of the Loftus and Masson method that allows for assessment of the circularity assumption.},
	language = {en},
	number = {3},
	urldate = {2019-02-08},
	journal = {Psychonomic Bulletin \& Review},
	author = {Franz, Volker H. and Loftus, Geoffrey R.},
	month = jun,
	year = {2012},
	pages = {395--404},
	file = {Franz and Loftus - 2012 - Standard errors and confidence intervals in within.pdf:/Users/YuhouZhou/Zotero/storage/LIDHRJX8/Franz and Loftus - 2012 - Standard errors and confidence intervals in within.pdf:application/pdf}
}

@article{melnik_dremel:_nodate,
	title = {Dremel: {Interactive} {Analysis} of {Web}-{Scale} {Datasets}},
	abstract = {Dremel is a scalable, interactive ad-hoc query system for analysis of read-only nested data. By combining multi-level execution trees and columnar data layout, it is capable of running aggregation queries over trillion-row tables in seconds. The system scales to thousands of CPUs and petabytes of data, and has thousands of users at Google. In this paper, we describe the architecture and implementation of Dremel, and explain how it complements MapReduce-based computing. We present a novel columnar storage representation for nested records and discuss experiments on few-thousand node instances of the system.},
	language = {en},
	author = {Melnik, Sergey and Gubarev, Andrey and Long, Jing Jing and Romer, Geoffrey and Shivakumar, Shiva and Tolton, Matt and Vassilakis, Theo},
	pages = {10},
	file = {Melnik et al. - Dremel Interactive Analysis of Web-Scale Datasets.pdf:/Users/YuhouZhou/Zotero/storage/GG28R89I/Melnik et al. - Dremel Interactive Analysis of Web-Scale Datasets.pdf:application/pdf}
}

@misc{noauthor_zotero_nodate,
	title = {Zotero {\textbar} {Your} personal research assistant},
	url = {https://www.zotero.org/start},
	urldate = {2019-03-07}
}

@article{lara_survey_2013,
	title = {A {Survey} on {Human} {Activity} {Recognition} using {Wearable} {Sensors}},
	volume = {15},
	issn = {1553-877X},
	url = {http://ieeexplore.ieee.org/document/6365160/},
	doi = {10.1109/SURV.2012.110112.00192},
	abstract = {Providing accurate and opportune information on people’s activities and behaviors is one of the most important tasks in pervasive computing. Innumerable applications can be visualized, for instance, in medical, security, entertainment, and tactical scenarios. Despite human activity recognition (HAR) being an active ﬁeld for more than a decade, there are still key aspects that, if addressed, would constitute a signiﬁcant turn in the way people interact with mobile devices. This paper surveys the state of the art in HAR based on wearable sensors. A general architecture is ﬁrst presented along with a description of the main components of any HAR system. We also propose a twolevel taxonomy in accordance to the learning approach (either supervised or semi-supervised) and the response time (either ofﬂine or online). Then, the principal issues and challenges are discussed, as well as the main solutions to each one of them. Twenty eight systems are qualitatively evaluated in terms of recognition performance, energy consumption, obtrusiveness, and ﬂexibility, among others. Finally, we present some open problems and ideas that, due to their high relevance, should be addressed in future research.},
	language = {en},
	number = {3},
	urldate = {2019-03-12},
	journal = {IEEE Communications Surveys \& Tutorials},
	author = {Lara, Oscar D. and Labrador, Miguel A.},
	year = {2013},
	pages = {1192--1209},
	file = {Lara and Labrador - 2013 - A Survey on Human Activity Recognition using Weara.pdf:/Users/YuhouZhou/Zotero/storage/UH2A6DQ4/Lara and Labrador - 2013 - A Survey on Human Activity Recognition using Weara.pdf:application/pdf}
}

@article{hartigan_algorithm_1979,
	title = {Algorithm {AS} 136: {A} {K}-{Means} {Clustering} {Algorithm}},
	volume = {28},
	issn = {0035-9254},
	shorttitle = {Algorithm {AS} 136},
	url = {https://www.jstor.org/stable/2346830},
	doi = {10.2307/2346830},
	number = {1},
	urldate = {2019-03-13},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Hartigan, J. A. and Wong, M. A.},
	year = {1979},
	pages = {100--108}
}

@article{hartigan_algorithm_1979-1,
	title = {Algorithm {AS} 136: {A} {K}-{Means} {Clustering} {Algorithm}},
	volume = {28},
	issn = {0035-9254},
	shorttitle = {Algorithm {AS} 136},
	url = {https://www.jstor.org/stable/2346830},
	doi = {10.2307/2346830},
	number = {1},
	urldate = {2019-03-13},
	journal = {Journal of the Royal Statistical Society. Series C (Applied Statistics)},
	author = {Hartigan, J. A. and Wong, M. A.},
	year = {1979},
	pages = {100--108},
	file = {JSTOR Full Text PDF:/Users/YuhouZhou/Zotero/storage/ALSJCFBI/Hartigan and Wong - 1979 - Algorithm AS 136 A K-Means Clustering Algorithm.pdf:application/pdf}
}

@article{yang_deep_nodate,
	title = {Deep {Convolutional} {Neural} {Networks} on {Multichannel} {Time} {Series} for {Human} {Activity} {Recognition}},
	abstract = {This paper focuses on human activity recognition (HAR) problem, in which inputs are multichannel time series signals acquired from a set of bodyworn inertial sensors and outputs are predeﬁned human activities. In this problem, extracting effective features for identifying activities is a critical but challenging task. Most existing work relies on heuristic hand-crafted feature design and shallow feature learning architectures, which cannot ﬁnd those distinguishing features to accurately classify different activities. In this paper, we propose a systematic feature learning method for HAR problem. This method adopts a deep convolutional neural networks (CNN) to automate feature learning from the raw inputs in a systematic way. Through the deep architecture, the learned features are deemed as the higher level abstract representation of low level raw time series signals. By leveraging the labelled information via supervised learning, the learned features are endowed with more discriminative power. Uniﬁed in one model, feature learning and classiﬁcation are mutually enhanced. All these unique advantages of the CNN make it outperform other HAR algorithms, as veriﬁed in the experiments on the Opportunity Activity Recognition Challenge and other benchmark datasets.},
	language = {en},
	author = {Yang, Jian Bo and Nguyen, Minh Nhut and San, Phyo Phyo and Li, Xiao Li and Krishnaswamy, Shonali},
	pages = {7},
	file = {Yang et al. - Deep Convolutional Neural Networks on Multichannel.pdf:/Users/YuhouZhou/Zotero/storage/48RBKTJK/Yang et al. - Deep Convolutional Neural Networks on Multichannel.pdf:application/pdf}
}

@article{buschken_sentence-based_2016,
	title = {Sentence-{Based} {Text} {Analysis} for {Customer} {Reviews}},
	volume = {35},
	issn = {0732-2399, 1526-548X},
	url = {http://pubsonline.informs.org/doi/10.1287/mksc.2016.0993},
	doi = {10.1287/mksc.2016.0993},
	abstract = {Firms collect an increasing amount of consumer feedback in the form of unstructured consumer reviews. These reviews contain text about consumer experiences with products and services that are di↵erent from surveys that query consumers for speciﬁc information. A challenge in analyzing unstructured consumer reviews is in making sense of the topics that are expressed in the words used to describe these experiences. We propose a new model for text analysis that makes use of the sentence structure contained in the reviews, and show that it leads to improved inference and prediction of consumer ratings relative to existing models using data from www.expedia.com and www.we8there.com. Sentencebased topics are found to be more distinguished and coherent than those identiﬁed from a word-based analysis.},
	language = {en},
	number = {6},
	urldate = {2019-07-17},
	journal = {Marketing Science},
	author = {Büschken, Joachim and Allenby, Greg M.},
	month = nov,
	year = {2016},
	pages = {953--975},
	file = {Büschken and Allenby - 2016 - Sentence-Based Text Analysis for Customer Reviews.pdf:/Users/YuhouZhou/Zotero/storage/9W8S3GMN/Büschken and Allenby - 2016 - Sentence-Based Text Analysis for Customer Reviews.pdf:application/pdf}
}

@article{jipeng_short_2019,
	title = {Short {Text} {Topic} {Modeling} {Techniques}, {Applications}, and {Performance}: {A} {Survey}},
	shorttitle = {Short {Text} {Topic} {Modeling} {Techniques}, {Applications}, and {Performance}},
	url = {http://arxiv.org/abs/1904.07695},
	abstract = {Analyzing short texts infers discriminative and coherent latent topics that is a critical and fundamental task since many real-world applications require semantic understanding of short texts. Traditional long text topic modeling algorithms (e.g., PLSA and LDA) based on word co-occurrences cannot solve this problem very well since only very limited word co-occurrence information is available in short texts. Therefore, short text topic modeling has already attracted much attention from the machine learning research community in recent years, which aims at overcoming the problem of sparseness in short texts. In this survey, we conduct a comprehensive review of various short text topic modeling techniques proposed in the literature. We present three categories of methods based on Dirichlet multinomial mixture, global word co-occurrences, and self-aggregation, with example of representative approaches in each category and analysis of their performance on various tasks. We develop the first comprehensive open-source library, called STTM, for use in Java that integrates all surveyed algorithms within a unified interface, benchmark datasets, to facilitate the expansion of new methods in this research field. Finally, we evaluate these state-of-the-art methods on many real-world datasets and compare their performance against one another and versus long text topic modeling algorithm.},
	urldate = {2019-07-17},
	journal = {arXiv:1904.07695 [cs]},
	author = {Jipeng, Qiang and Zhenyu, Qian and Yun, Li and Yunhao, Yuan and Xindong, Wu},
	month = apr,
	year = {2019},
	note = {arXiv: 1904.07695},
	keywords = {Computer Science - Computation and Language, Computer Science - Information Retrieval},
	annote = {Comment: arXiv admin note: text overlap with arXiv:1808.02215 by other authors},
	file = {arXiv\:1904.07695 PDF:/Users/YuhouZhou/Zotero/storage/S7YMGZ85/Jipeng et al. - 2019 - Short Text Topic Modeling Techniques, Applications.pdf:application/pdf;arXiv.org Snapshot:/Users/YuhouZhou/Zotero/storage/ZGXIXJDM/1904.html:text/html}
}

@article{hofmann_probabilistic_2017,
	title = {Probabilistic {Latent} {Semantic} {Indexing}},
	volume = {51},
	abstract = {Probabilistic Latent Semantic Indexing is a novel approach to automated document indexing which is based on a statistical latent class model for factor analysis of count data. Fitted from a training corpus of text documents by a generalization of the Expectation Maximization algorithm, the utilized model is able to deal with domain speci c synonymy as well as with polysemous words. In contrast to standard Latent Semantic Indexing  LSI  by Singular Value Decomposition, the probabilistic variant has a solid statistical foundation and de nes a proper generative data model. Retrieval experiments on a number of test collections indicate substantial performance gains over direct term matching methods as well as over LSI. In particular, the combination of models with di erent dimensionalities has proven to be advantageous.},
	language = {en},
	number = {2},
	journal = {ACM SIGIR Forum},
	author = {Hofmann, Thomas},
	year = {2017},
	pages = {8},
	file = {Hofmann - 2017 - Probabilistic Latent Semantic Indexing.pdf:/Users/YuhouZhou/Zotero/storage/XD5S2LPP/Hofmann - 2017 - Probabilistic Latent Semantic Indexing.pdf:application/pdf}
}

@inproceedings{lund_cross-referencing_2019,
	address = {Minneapolis, Minnesota},
	title = {Cross-referencing {Using} {Fine}-grained {Topic} {Modeling}},
	url = {https://www.aclweb.org/anthology/N19-1399},
	abstract = {Cross-referencing, which links passages of text to other related passages, can be a valuable study aid for facilitating comprehension of a text. However, cross-referencing requires first, a comprehensive thematic knowledge of the entire corpus, and second, a focused search through the corpus specifically to find such useful connections. Due to this, cross-reference resources are prohibitively expensive and exist only for the most well-studied texts (e.g. religious texts). We develop a topic-based system for automatically producing candidate cross-references which can be easily verified by human annotators. Our system utilizes fine-grained topic modeling with thousands of highly nuanced and specific topics to identify verse pairs which are topically related. We demonstrate that our system can be cost effective compared to having annotators acquire the expertise necessary to produce cross-reference resources unaided.},
	urldate = {2019-07-17},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lund, Jeffrey and Armstrong, Piper and Fearn, Wilson and Cowley, Stephen and Hales, Emily and Seppi, Kevin},
	month = jun,
	year = {2019},
	pages = {3978--3987},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/7CE6MQ9W/Lund et al. - 2019 - Cross-referencing Using Fine-grained Topic Modelin.pdf:application/pdf}
}

@misc{noauthor_topic_2019,
	title = {Topic model},
	copyright = {Creative Commons Attribution-ShareAlike License},
	url = {https://en.wikipedia.org/w/index.php?title=Topic_model&oldid=899045268},
	abstract = {In machine learning and natural language processing, a topic model is a type of statistical model for discovering the abstract "topics" that occur in a collection of documents. Topic modeling is a frequently used text-mining tool for discovery of hidden semantic structures in a text body. Intuitively, given that a document is about a particular topic, one would expect particular words to appear in the document more or less frequently: "dog" and "bone" will appear more often in documents about dogs, "cat" and "meow" will appear in documents about cats, and "the" and "is" will appear equally in both. A document typically concerns multiple topics in different proportions; thus, in a document that is 10\% about cats and 90\% about dogs, there would probably be about 9 times more dog words than cat words. The "topics" produced by topic modeling techniques are clusters of similar words. A topic model captures this intuition in a mathematical framework, which allows examining a set of documents and discovering, based on the statistics of the words in each, what the topics might be and what each document's balance of topics is.
Topic models are also referred to as probabilistic topic models, which refers to statistical algorithms for discovering the latent semantic structures of an extensive text body. In the age of information, the amount of the written material we encounter each day is simply beyond our processing capacity. Topic models can help to organize and offer insights for us to understand large collections of unstructured text bodies. Originally developed as a text-mining tool, topic models have been used to detect instructive structures in data such as genetic information, images, and networks. They also have applications in other fields such as bioinformatics.},
	language = {en},
	urldate = {2019-07-17},
	journal = {Wikipedia},
	month = may,
	year = {2019},
	note = {Page Version ID: 899045268},
	file = {Snapshot:/Users/YuhouZhou/Zotero/storage/PGZAPPHN/index.html:text/html}
}

@article{blei_probabilistic_2012,
	title = {Probabilistic topic models},
	volume = {55},
	issn = {00010782},
	url = {http://dl.acm.org/citation.cfm?doid=2133806.2133826},
	doi = {10.1145/2133806.2133826},
	language = {en},
	number = {4},
	urldate = {2019-07-17},
	journal = {Communications of the ACM},
	author = {Blei, David M.},
	month = apr,
	year = {2012},
	pages = {77},
	file = {Blei - 2012 - Probabilistic topic models.pdf:/Users/YuhouZhou/Zotero/storage/PFHMR7N6/Blei - 2012 - Probabilistic topic models.pdf:application/pdf}
}

@inproceedings{jin_combining_2018,
	address = {New Orleans, Louisiana},
	title = {Combining {Deep} {Learning} and {Topic} {Modeling} for {Review} {Understanding} in {Context}-{Aware} {Recommendation}},
	url = {https://www.aclweb.org/anthology/N18-1145},
	doi = {10.18653/v1/N18-1145},
	abstract = {With the rise of e-commerce, people are accustomed to writing their reviews after receiving the goods. These comments are so important that a bad review can have a direct impact on others buying. Besides, the abundant information within user reviews is very useful for extracting user preferences and item properties. In this paper, we investigate the approach to effectively utilize review information for recommender systems. The proposed model is named LSTM-Topic matrix factorization (LTMF) which integrates both LSTM and Topic Modeling for review understanding. In the experiments on popular review dataset Amazon , our LTMF model outperforms previous proposed HFT model and ConvMF model in rating prediction. Furthermore, LTMF shows the better ability on making topic clustering than traditional topic model based method, which implies integrating the information from deep learning and topic modeling is a meaningful approach to make a better understanding of reviews.},
	urldate = {2019-07-17},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Jin, Mingmin and Luo, Xin and Zhu, Huiling and Zhuo, Hankz Hankui},
	month = jun,
	year = {2018},
	pages = {1605--1614},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/KWVNYG4T/Jin et al. - 2018 - Combining Deep Learning and Topic Modeling for Rev.pdf:application/pdf}
}

@misc{noauthor_jira_nodate,
	title = {{JIRA} · {Services} · {Settings} · {Zhou}, {Yuhou} / dummy\_ci},
	url = {http://git.cewe.lan/zhouy/dummy_ci/services/jira/edit},
	abstract = {CEWE Code Server},
	language = {en},
	urldate = {2019-07-17},
	journal = {GitLab},
	file = {Snapshot:/Users/YuhouZhou/Zotero/storage/YQQYISGJ/edit.html:text/html}
}

@inproceedings{li_topic_2016,
	address = {Pisa, Italy},
	title = {Topic {Modeling} for {Short} {Texts} with {Auxiliary} {Word} {Embeddings}},
	isbn = {978-1-4503-4069-4},
	url = {http://dl.acm.org/citation.cfm?doid=2911451.2911499},
	doi = {10.1145/2911451.2911499},
	abstract = {For many applications that require semantic understanding of short texts, inferring discriminative and coherent latent topics from short texts is a critical and fundamental task. Conventional topic models largely rely on word co-occurrences to derive topics from a collection of documents. However, due to the length of each document, short texts are much more sparse in terms of word co-occurrences. Data sparsity therefore becomes a bottleneck for conventional topic models to achieve good results on short texts. On the other hand, when a human being interprets a piece of short text, the understanding is not solely based on its content words, but also her background knowledge (e.g., semantically related words). The recent advances in word embedding offer effective learning of word semantic relations from a large corpus. Exploiting such auxiliary word embeddings to enrich topic modeling for short texts is the main focus of this paper. To this end, we propose a simple, fast, and effective topic model for short texts, named GPU-DMM. Based on the Dirichlet Multinomial Mixture (DMM) model, GPU-DMM promotes the semantically related words under the same topic during the sampling process by using the generalized Po´lya urn (GPU) model. In this sense, the background knowledge about word semantic relatedness learned from millions of external documents can be easily exploited to improve topic modeling for short texts. Through extensive experiments on two real-world short text collections in two languages, we show that GPU-DMM achieves comparable or better topic representations than state-of-the-art models, measured by topic coherence. The learned topic representation leads to the best accuracy in text classiﬁcation task, which is used as an indirect evaluation.},
	language = {en},
	urldate = {2019-07-17},
	booktitle = {Proceedings of the 39th {International} {ACM} {SIGIR} conference on {Research} and {Development} in {Information} {Retrieval} - {SIGIR} '16},
	publisher = {ACM Press},
	author = {Li, Chenliang and Wang, Haoran and Zhang, Zhiqian and Sun, Aixin and Ma, Zongyang},
	year = {2016},
	pages = {165--174}
}

@incollection{mcauliffe_supervised_2008,
	title = {Supervised {Topic} {Models}},
	url = {http://papers.nips.cc/paper/3328-supervised-topic-models.pdf},
	urldate = {2019-07-17},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 20},
	publisher = {Curran Associates, Inc.},
	author = {Mcauliffe, Jon D. and Blei, David M.},
	editor = {Platt, J. C. and Koller, D. and Singer, Y. and Roweis, S. T.},
	year = {2008},
	pages = {121--128},
	file = {NIPS Full Text PDF:/Users/YuhouZhou/Zotero/storage/IM8E9YHR/Mcauliffe and Blei - 2008 - Supervised Topic Models.pdf:application/pdf;NIPS Snapshot:/Users/YuhouZhou/Zotero/storage/BHWQAFHN/3328-supervised-topic-models.html:text/html}
}

@inproceedings{ramage_labeled_2009,
	address = {Singapore},
	title = {Labeled {LDA}: {A} supervised topic model for credit attribution in multi-labeled corpora},
	volume = {1},
	isbn = {978-1-932432-59-6},
	shorttitle = {Labeled {LDA}},
	url = {http://portal.acm.org/citation.cfm?doid=1699510.1699543},
	doi = {10.3115/1699510.1699543},
	abstract = {A signiﬁcant portion of the world’s text is tagged by readers on social bookmarking websites. Credit attribution is an inherent problem in these corpora because most pages have multiple tags, but the tags do not always apply with equal speciﬁcity across the whole document. Solving the credit attribution problem requires associating each word in a document with the most appropriate tags and vice versa. This paper introduces Labeled LDA, a topic model that constrains Latent Dirichlet Allocation by deﬁning a one-to-one correspondence between LDA’s latent topics and user tags. This allows Labeled LDA to directly learn word-tag correspondences. We demonstrate Labeled LDA’s improved expressiveness over traditional LDA with visualizations of a corpus of tagged web pages from del.icio.us. Labeled LDA outperforms SVMs by more than 3 to 1 when extracting tag-speciﬁc document snippets. As a multi-label text classiﬁer, our model is competitive with a discriminative baseline on a variety of datasets.},
	language = {en},
	urldate = {2019-07-17},
	booktitle = {Proceedings of the 2009 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} {Volume} 1 - {EMNLP} '09},
	publisher = {Association for Computational Linguistics},
	author = {Ramage, Daniel and Hall, David and Nallapati, Ramesh and Manning, Christopher D.},
	year = {2009},
	pages = {248},
	file = {Ramage et al. - 2009 - Labeled LDA a supervised topic model for credit a.pdf:/Users/YuhouZhou/Zotero/storage/ITRF45TC/Ramage et al. - 2009 - Labeled LDA a supervised topic model for credit a.pdf:application/pdf}
}

@article{teh_hierarchical_2006,
	title = {Hierarchical {Dirichlet} {Processes}},
	volume = {101},
	issn = {0162-1459, 1537-274X},
	url = {http://www.tandfonline.com/doi/abs/10.1198/016214506000000302},
	doi = {10.1198/016214506000000302},
	abstract = {We consider problems involving groups of data, where each observation within a group is a draw from a mixture model, and where it is desirable to share mixture components between groups. We assume that the number of mixture components is unknown a priori and is to be inferred from the data. In this setting it is natural to consider sets of Dirichlet processes, one for each group, where the well-known clustering property of the Dirichlet process provides a nonparametric prior for the number of mixture components within each group. Given our desire to tie the mixture models in the various groups, we consider a hierarchical model, speciﬁcally one in which the base measure for the child Dirichlet processes is itself distributed according to a Dirichlet process. Such a base measure being discrete, the child Dirichlet processes necessarily share atoms. Thus, as desired, the mixture models in the different groups necessarily share mixture components. We discuss representations of hierarchical Dirichlet processes in terms of a stick-breaking process, and a generalization of the Chinese restaurant process that we refer to as the “Chinese restaurant franchise.” We present Markov chain Monte Carlo algorithms for posterior inference in hierarchical Dirichlet process mixtures, and describe applications to problems in information retrieval and text modelling.},
	language = {en},
	number = {476},
	urldate = {2019-07-16},
	journal = {Journal of the American Statistical Association},
	author = {Teh, Yee Whye and Jordan, Michael I and Beal, Matthew J and Blei, David M},
	month = dec,
	year = {2006},
	pages = {1566--1581},
	file = {Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:/Users/YuhouZhou/Zotero/storage/K4594PTU/Teh et al. - 2006 - Hierarchical Dirichlet Processes.pdf:application/pdf}
}

@inproceedings{rangarajan_sridhar_unsupervised_2015,
	address = {Denver, Colorado},
	title = {Unsupervised {Topic} {Modeling} for {Short} {Texts} {Using} {Distributed} {Representations} of {Words}},
	url = {https://www.aclweb.org/anthology/W15-1526},
	doi = {10.3115/v1/W15-1526},
	urldate = {2019-07-16},
	booktitle = {Proceedings of the 1st {Workshop} on {Vector} {Space} {Modeling} for {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Rangarajan Sridhar, Vivek Kumar},
	month = jun,
	year = {2015},
	pages = {192--200},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/M7V7WL8U/Rangarajan Sridhar - 2015 - Unsupervised Topic Modeling for Short Texts Using .pdf:application/pdf}
}

@article{lafferty_david_nodate,
	title = {{DAVID} {M}. {BLEI} {PRINCETON} {UNIVERSITY}},
	language = {en},
	author = {Lafferty, John D},
	pages = {24},
	file = {Lafferty - DAVID M. BLEI PRINCETON UNIVERSITY.pdf:/Users/YuhouZhou/Zotero/storage/HSLV6UZK/Lafferty - DAVID M. BLEI PRINCETON UNIVERSITY.pdf:application/pdf}
}

@inproceedings{ramage_partially_2011,
	address = {New York, NY, USA},
	series = {{KDD} '11},
	title = {Partially {Labeled} {Topic} {Models} for {Interpretable} {Text} {Mining}},
	isbn = {978-1-4503-0813-7},
	url = {http://doi.acm.org/10.1145/2020408.2020481},
	doi = {10.1145/2020408.2020481},
	abstract = {Abstract Much of the world's electronic text is annotated with human-interpretable labels, such as tags on web pages and subject codes on academic publications. Effective text mining in this setting requires models that can flexibly account for the textual patterns that underlie the observed labels while still discovering unlabeled topics. Neither supervised classification, with its focus on label prediction, nor purely unsupervised learning, which does not model the labels explicitly, is appropriate. In this paper, we present two new partially supervised generative models of labeled text, Partially Labeled Dirichlet Allocation (PLDA) and the Partially Labeled Dirichlet Process (PLDP). These models make use of the unsupervised learning machinery of topic models to discover the hidden topics within each label, as well as unlabeled, corpus-wide latent topics. We explore applications with qualitative case studies of tagged web pages from del.icio.us and PhD dissertation abstracts, demonstrating improved model interpretability over traditional topic models. We use the many tags present in our del.icio.us dataset to quantitatively demonstrate the new models' higher correlation with human relatedness scores over several strong baselines.},
	urldate = {2019-07-16},
	booktitle = {Proceedings of the 17th {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Ramage, Daniel and Manning, Christopher D. and Dumais, Susan},
	year = {2011},
	note = {event-place: San Diego, California, USA},
	keywords = {partially supervised learning, text mining},
	pages = {457--465}
}

@inproceedings{stevens_exploring_2012,
	address = {Jeju Island, Korea},
	title = {Exploring {Topic} {Coherence} over {Many} {Models} and {Many} {Topics}},
	url = {https://www.aclweb.org/anthology/D12-1087},
	urldate = {2019-07-16},
	booktitle = {Proceedings of the 2012 {Joint} {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and {Computational} {Natural} {Language} {Learning}},
	publisher = {Association for Computational Linguistics},
	author = {Stevens, Keith and Kegelmeyer, Philip and Andrzejewski, David and Buttler, David},
	month = jul,
	year = {2012},
	pages = {952--961},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/B6FU6SB9/Stevens et al. - 2012 - Exploring Topic Coherence over Many Models and Man.pdf:application/pdf}
}

@inproceedings{wallach_evaluation_2009,
	address = {Montreal, Quebec, Canada},
	title = {Evaluation methods for topic models},
	isbn = {978-1-60558-516-1},
	url = {http://portal.acm.org/citation.cfm?doid=1553374.1553515},
	doi = {10.1145/1553374.1553515},
	abstract = {A natural evaluation metric for statistical topic models is the probability of held-out documents given a trained model. While exact computation of this probability is intractable, several estimators for this probability have been used in the topic modeling literature, including the harmonic mean method and empirical likelihood method. In this paper, we demonstrate experimentally that commonly-used methods are unlikely to accurately estimate the probability of heldout documents, and propose two alternative methods that are both accurate and eﬃcient.},
	language = {en},
	urldate = {2019-07-16},
	booktitle = {Proceedings of the 26th {Annual} {International} {Conference} on {Machine} {Learning} - {ICML} '09},
	publisher = {ACM Press},
	author = {Wallach, Hanna M. and Murray, Iain and Salakhutdinov, Ruslan and Mimno, David},
	year = {2009},
	pages = {1--8},
	file = {Wallach et al. - 2009 - Evaluation methods for topic models.pdf:/Users/YuhouZhou/Zotero/storage/HDKB555B/Wallach et al. - 2009 - Evaluation methods for topic models.pdf:application/pdf}
}

@inproceedings{roder_exploring_2015,
	address = {Shanghai, China},
	title = {Exploring the {Space} of {Topic} {Coherence} {Measures}},
	isbn = {978-1-4503-3317-7},
	url = {http://dl.acm.org/citation.cfm?doid=2684822.2685324},
	doi = {10.1145/2684822.2685324},
	abstract = {Quantifying the coherence of a set of statements is a long standing problem with many potential applications that has attracted researchers from diﬀerent sciences. The special case of measuring coherence of topics has been recently studied to remedy the problem that topic models give no guaranty on the interpretablity of their output. Several benchmark datasets were produced that record human judgements of the interpretability of topics. We are the ﬁrst to propose a framework that allows to construct existing word based coherence measures as well as new ones by combining elementary components. We conduct a systematic search of the space of coherence measures using all publicly available topic relevance data for the evaluation. Our results show that new combinations of components outperform existing measures with respect to correlation to human ratings. Finally, we outline how our results can be transferred to further applications in the context of text mining, information retrieval and the world wide web.},
	language = {en},
	urldate = {2019-07-16},
	booktitle = {Proceedings of the {Eighth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining} - {WSDM} '15},
	publisher = {ACM Press},
	author = {Röder, Michael and Both, Andreas and Hinneburg, Alexander},
	year = {2015},
	pages = {399--408},
	file = {Röder et al. - 2015 - Exploring the Space of Topic Coherence Measures.pdf:/Users/YuhouZhou/Zotero/storage/57K6ZQSR/Röder et al. - 2015 - Exploring the Space of Topic Coherence Measures.pdf:application/pdf}
}

@misc{eisenstein_course_2019,
	title = {Course materials for {Georgia} {Tech} {CS} 4650 and 7650, "{Natural} {Language}": jacobeisenstein/gt-nlp-class},
	shorttitle = {Course materials for {Georgia} {Tech} {CS} 4650 and 7650, "{Natural} {Language}"},
	url = {https://github.com/jacobeisenstein/gt-nlp-class},
	urldate = {2019-07-16},
	author = {Eisenstein, Jacob},
	month = jul,
	year = {2019},
	note = {original-date: 2013-02-28T18:27:27Z}
}

@article{blei_latent_2003,
	title = {Latent {Dirichlet} {Allocation}},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v3/blei03a.html},
	number = {Jan},
	urldate = {2019-07-16},
	journal = {Journal of Machine Learning Research},
	author = {Blei, David M. and Ng, Andrew Y. and Jordan, Michael I.},
	year = {2003},
	pages = {993--1022},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/87BJHAS9/Blei et al. - 2003 - Latent Dirichlet Allocation.pdf:application/pdf;Snapshot:/Users/YuhouZhou/Zotero/storage/FQPJPZUP/blei03a.html:text/html}
}

@article{deerwester_indexing_1990,
	title = {Indexing by latent semantic analysis},
	volume = {41},
	issn = {0002-8231, 1097-4571},
	url = {http://doi.wiley.com/10.1002/%28SICI%291097-4571%28199009%2941%3A6%3C391%3A%3AAID-ASI1%3E3.0.CO%3B2-9},
	doi = {10.1002/(SICI)1097-4571(199009)41:6<391::AID-ASI1>3.0.CO;2-9},
	abstract = {A new method for automatic indexing and retrieval is described. The approach is to take advantage of implicit higher-order structure in the association of terms with documents ("semantic structure") in order to improve the detection of relevant documents on the basis of terms found in queries. The particular technique used is singular-value decomposition, in which a large term by document matrix is decomposed into a set of ca 100 orthogonal factors from which the original matrix can be approximated by linear combination. Documents are represented by ca 100 item vectors of factor weights. Queries are represented as pseudo-document vectors formed from weighted combinations of terms, and documents with supra-threshold cosine values are returned. Initial tests find this completely automatic method for retrieval to be promising.},
	language = {en},
	number = {6},
	urldate = {2019-07-24},
	journal = {Journal of the American Society for Information Science},
	author = {Deerwester, Scott and Dumais, Susan T. and Furnas, George W. and Landauer, Thomas K. and Harshman, Richard},
	month = sep,
	year = {1990},
	pages = {391--407},
	file = {Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:/Users/YuhouZhou/Zotero/storage/QKPGHV28/Deerwester et al. - 1990 - Indexing by latent semantic analysis.pdf:application/pdf}
}

@article{cortes_support-vector_1995,
	title = {Support-vector networks},
	volume = {20},
	issn = {0885-6125, 1573-0565},
	url = {http://link.springer.com/10.1007/BF00994018},
	doi = {10.1007/BF00994018},
	abstract = {The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very highdimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.},
	language = {en},
	number = {3},
	urldate = {2019-07-24},
	journal = {Machine Learning},
	author = {Cortes, Corinna and Vapnik, Vladimir},
	month = sep,
	year = {1995},
	pages = {273--297},
	file = {Cortes and Vapnik - 1995 - Support-vector networks.pdf:/Users/YuhouZhou/Zotero/storage/XJ7BCQT9/Cortes and Vapnik - 1995 - Support-vector networks.pdf:application/pdf}
}

@article{van_de_schoot_gentle_2014,
	title = {A {Gentle} {Introduction} to {Bayesian} {Analysis}: {Applications} to {Developmental} {Research}},
	volume = {85},
	issn = {00093920},
	shorttitle = {A {Gentle} {Introduction} to {Bayesian} {Analysis}},
	url = {http://doi.wiley.com/10.1111/cdev.12169},
	doi = {10.1111/cdev.12169},
	language = {en},
	number = {3},
	urldate = {2019-07-24},
	journal = {Child Development},
	author = {van de Schoot, Rens and Kaplan, David and Denissen, Jaap and Asendorpf, Jens B. and Neyer, Franz J. and van Aken, Marcel A.G.},
	month = may,
	year = {2014},
	pages = {842--860},
	file = {van de Schoot et al. - 2014 - A Gentle Introduction to Bayesian Analysis Applic.pdf:/Users/YuhouZhou/Zotero/storage/MBS7P29R/van de Schoot et al. - 2014 - A Gentle Introduction to Bayesian Analysis Applic.pdf:application/pdf}
}

@article{fienberg_when_2006,
	title = {When did {Bayesian} inference become "{Bayesian}"?},
	volume = {1},
	issn = {1936-0975},
	url = {http://projecteuclid.org/euclid.ba/1340371071},
	doi = {10.1214/06-BA101},
	abstract = {While Bayes’ theorem has a 250-year history, and the method of inverse probability that ﬂowed from it dominated statistical thinking into the twentieth century, the adjective “Bayesian” was not part of the statistical lexicon until relatively recently. This paper provides an overview of key Bayesian developments, beginning with Bayes’ posthumously published 1763 paper and continuing up through approximately 1970, including the period of time when “Bayesian” emerged as the label of choice for those who advocated Bayesian methods.},
	language = {en},
	number = {1},
	urldate = {2019-07-24},
	journal = {Bayesian Analysis},
	author = {Fienberg, Stephen E.},
	month = mar,
	year = {2006},
	pages = {1--40},
	file = {Fienberg - 2006 - When did Bayesian inference become Bayesian.pdf:/Users/YuhouZhou/Zotero/storage/MZEXL695/Fienberg - 2006 - When did Bayesian inference become Bayesian.pdf:application/pdf}
}

@inproceedings{weng_twitterrank:_2010,
	address = {New York, New York, USA},
	title = {{TwitterRank}: finding topic-sensitive influential twitterers},
	isbn = {978-1-60558-889-6},
	shorttitle = {{TwitterRank}},
	url = {http://portal.acm.org/citation.cfm?doid=1718487.1718520},
	doi = {10.1145/1718487.1718520},
	abstract = {This paper focuses on the problem of identifying inﬂuential users of micro-blogging services. Twitter, one of the most notable micro-blogging services, employs a social-networking model called “following”, in which each user can choose who she wants to “follow” to receive tweets from without requiring the latter to give permission ﬁrst. In a dataset prepared for this study, it is observed that (1) 72.4\% of the users in Twitter follow more than 80\% of their followers, and (2) 80.5\% of the users have 80\% of users they are following follow them back. Our study reveals that the presence of “reciprocity” can be explained by phenomenon of homophily [14]. Based on this ﬁnding, TwitterRank, an extension of PageRank algorithm, is proposed to measure the inﬂuence of users in Twitter. TwitterRank measures the inﬂuence taking both the topical similarity between users and the link structure into account. Experimental results show that TwitterRank outperforms the one Twitter currently uses and other related algorithms, including the original PageRank and Topic-sensitive PageRank.},
	language = {en},
	urldate = {2019-07-24},
	booktitle = {Proceedings of the third {ACM} international conference on {Web} search and data mining - {WSDM} '10},
	publisher = {ACM Press},
	author = {Weng, Jianshu and Lim, Ee-Peng and Jiang, Jing and He, Qi},
	year = {2010},
	pages = {261},
	file = {Weng et al. - 2010 - TwitterRank finding topic-sensitive influential t.pdf:/Users/YuhouZhou/Zotero/storage/QSRZ4CG7/Weng et al. - 2010 - TwitterRank finding topic-sensitive influential t.pdf:application/pdf}
}

@inproceedings{mehrotra_improving_2013,
	address = {Dublin, Ireland},
	title = {Improving {LDA} topic models for microblogs via tweet pooling and automatic labeling},
	isbn = {978-1-4503-2034-4},
	url = {http://dl.acm.org/citation.cfm?doid=2484028.2484166},
	doi = {10.1145/2484028.2484166},
	abstract = {Twitter, or the world of 140 characters poses serious challenges to the efﬁcacy of topic models on short, messy text. While topic models such as Latent Dirichlet Allocation (LDA) have a long history of successful application to news articles and academic abstracts, they are often less coherent when applied to microblog content like Twitter. In this paper, we investigate methods to improve topics learned from Twitter content without modifying the basic machinery of LDA; we achieve this through various pooling schemes that aggregate tweets in a data preprocessing step for LDA. We empirically establish that a novel method of tweet pooling by hashtags leads to a vast improvement in a variety of measures for topic coherence across three diverse Twitter datasets in comparison to an unmodiﬁed LDA baseline and a variety of pooling schemes. An additional contribution of automatic hashtag labeling further improves on the hashtag pooling results for a subset of metrics. Overall, these two novel schemes lead to signiﬁcantly improved LDA topic models on Twitter content.},
	language = {en},
	urldate = {2019-07-24},
	booktitle = {Proceedings of the 36th international {ACM} {SIGIR} conference on {Research} and development in information retrieval - {SIGIR} '13},
	publisher = {ACM Press},
	author = {Mehrotra, Rishabh and Sanner, Scott and Buntine, Wray and Xie, Lexing},
	year = {2013},
	pages = {889},
	file = {Mehrotra et al. - 2013 - Improving LDA topic models for microblogs via twee.pdf:/Users/YuhouZhou/Zotero/storage/VDGKZTQN/Mehrotra et al. - 2013 - Improving LDA topic models for microblogs via twee.pdf:application/pdf}
}

@inproceedings{hong_empirical_2010,
	address = {Washington D.C., District of Columbia},
	title = {Empirical study of topic modeling in {Twitter}},
	isbn = {978-1-4503-0217-3},
	url = {http://portal.acm.org/citation.cfm?doid=1964858.1964870},
	doi = {10.1145/1964858.1964870},
	abstract = {Social networks such as Facebook, LinkedIn, and Twitter have been a crucial source of information for a wide spectrum of users. In Twitter, popular information that is deemed important by the community propagates through the network. Studying the characteristics of content in the messages becomes important for a number of tasks, such as breaking news detection, personalized message recommendation, friends recommendation, sentiment analysis and others. While many researchers wish to use standard text mining tools to understand messages on Twitter, the restricted length of those messages prevents them from being employed to their full potential.},
	language = {en},
	urldate = {2019-07-24},
	booktitle = {Proceedings of the {First} {Workshop} on {Social} {Media} {Analytics} - {SOMA} '10},
	publisher = {ACM Press},
	author = {Hong, Liangjie and Davison, Brian D.},
	year = {2010},
	pages = {80--88},
	file = {Hong and Davison - 2010 - Empirical study of topic modeling in Twitter.pdf:/Users/YuhouZhou/Zotero/storage/SXWVIDRJ/Hong and Davison - 2010 - Empirical study of topic modeling in Twitter.pdf:application/pdf}
}

@misc{noauthor_1_nodate,
	title = {(1 封私信) {LDA适合单条句子级别的短文本分类吗}？ - 知乎},
	url = {https://www.zhihu.com/question/313275693/answer/605783460},
	urldate = {2019-07-23},
	file = {(1 封私信) LDA适合单条句子级别的短文本分类吗？ - 知乎:/Users/YuhouZhou/Zotero/storage/7DVQ94WL/605783460.html:text/html}
}

@misc{noauthor_1_nodate-1,
	title = {(1 封私信) {LDA}(latent dirichlet allocation)在处理短文本时该如何优化? - 知乎},
	url = {https://www.zhihu.com/question/30382384/answer/49749184},
	urldate = {2019-07-23},
	file = {(1 封私信) LDA(latent dirichlet allocation)在处理短文本时该如何优化? - 知乎:/Users/YuhouZhou/Zotero/storage/HDK8D3DF/49749184.html:text/html}
}

@misc{noauthor_1_nodate-2,
	title = {(1 封私信) {LDA在短文本分类方面的扩展模型有哪些}？ - 知乎},
	url = {https://www.zhihu.com/question/21504444/answer/44304317},
	urldate = {2019-07-23}
}

@misc{yan_code_2019,
	title = {Code for {Biterm} {Topic} {Model} (published in {WWW} 2013): xiaohuiyan/{BTM}},
	copyright = {Apache-2.0},
	shorttitle = {Code for {Biterm} {Topic} {Model} (published in {WWW} 2013)},
	url = {https://github.com/xiaohuiyan/BTM},
	urldate = {2019-07-23},
	author = {Yan, Xiaohui},
	month = jul,
	year = {2019},
	note = {original-date: 2015-01-10T05:16:28Z}
}

@article{brody_unsupervised_nodate,
	title = {An unsupervised aspect-sentiment model for online reviews},
	abstract = {With the increase in popularity of online review sites comes a corresponding need for tools capable of extracting the information most important to the user from the plain text data. Due to the diversity in products and services being reviewed, supervised methods are often not practical. We present an unsupervised system for extracting aspects and determining sentiment in review text. The method is simple and ﬂexible with regard to domain and language, and takes into account the inﬂuence of aspect on sentiment polarity, an issue largely ignored in previous literature. We demonstrate its effectiveness on both component tasks, where it achieves similar results to more complex semi-supervised methods that are restricted by their reliance on manual annotation and extensive knowledge sources.},
	language = {en},
	author = {Brody, Samuel and Elhadad, Noemie},
	pages = {9},
	file = {Brody and Elhadad - An unsupervised aspect-sentiment model for online .pdf:/Users/YuhouZhou/Zotero/storage/5ZZEL7SE/Brody and Elhadad - An unsupervised aspect-sentiment model for online .pdf:application/pdf}
}

@inproceedings{yin_dirichlet_2014,
	address = {New York, New York, USA},
	title = {A dirichlet multinomial mixture model-based approach for short text clustering},
	isbn = {978-1-4503-2956-9},
	url = {http://dl.acm.org/citation.cfm?doid=2623330.2623715},
	doi = {10.1145/2623330.2623715},
	abstract = {Short text clustering has become an increasingly important task with the popularity of social media like Twitter, Google+, and Facebook. It is a challenging problem due to its sparse, high-dimensional, and large-volume characteristics. In this paper, we proposed a collapsed Gibbs Sampling algorithm for the Dirichlet Multinomial Mixture model for short text clustering (abbr. to GSDMM). We found that GSDMM can infer the number of clusters automatically with a good balance between the completeness and homogeneity of the clustering results, and is fast to converge. GSDMM can also cope with the sparse and high-dimensional problem of short texts, and can obtain the representative words of each cluster. Our extensive experimental study shows that GSDMM can achieve signiﬁcantly better performance than three other clustering models.},
	language = {en},
	urldate = {2019-07-22},
	booktitle = {Proceedings of the 20th {ACM} {SIGKDD} international conference on {Knowledge} discovery and data mining - {KDD} '14},
	publisher = {ACM Press},
	author = {Yin, Jianhua and Wang, Jianyong},
	year = {2014},
	pages = {233--242},
	file = {Yin and Wang - 2014 - A dirichlet multinomial mixture model-based approa.pdf:/Users/YuhouZhou/Zotero/storage/PYJKPMKS/Yin and Wang - 2014 - A dirichlet multinomial mixture model-based approa.pdf:application/pdf}
}

@misc{atefm_python_2019,
	title = {Python implemetation for {Dirichlet} {Multinomial} {Mixture} ({DMM}) model: atefm/{pDMM}},
	shorttitle = {Python implemetation for {Dirichlet} {Multinomial} {Mixture} ({DMM}) model},
	url = {https://github.com/atefm/pDMM},
	urldate = {2019-07-22},
	author = {atefm},
	month = jul,
	year = {2019},
	note = {original-date: 2017-03-05T10:24:55Z}
}

@article{li_enhancing_2017,
	title = {Enhancing {Topic} {Modeling} for {Short} {Texts} with {Auxiliary} {Word} {Embeddings}},
	volume = {36},
	issn = {10468188},
	url = {http://dl.acm.org/citation.cfm?doid=3133943.3091108},
	doi = {10.1145/3091108},
	language = {en},
	number = {2},
	urldate = {2019-07-26},
	journal = {ACM Transactions on Information Systems},
	author = {Li, Chenliang and Duan, Yu and Wang, Haoran and Zhang, Zhiqian and Sun, Aixin and Ma, Zongyang},
	month = aug,
	year = {2017},
	pages = {1--30},
	file = {Li et al. - 2017 - Enhancing Topic Modeling for Short Texts with Auxi.pdf:/Users/YuhouZhou/Zotero/storage/W5F93CBN/Li et al. - 2017 - Enhancing Topic Modeling for Short Texts with Auxi.pdf:application/pdf}
}

@inproceedings{sievert_ldavis:_2014,
	address = {Baltimore, Maryland, USA},
	title = {{LDAvis}: {A} method for visualizing and interpreting topics},
	shorttitle = {{LDAvis}},
	url = {http://aclweb.org/anthology/W14-3110},
	doi = {10.3115/v1/W14-3110},
	abstract = {We present LDAvis, a web-based interactive visualization of topics estimated using Latent Dirichlet Allocation that is built using a combination of R and D3. Our visualization provides a global view of the topics (and how they differ from each other), while at the same time allowing for a deep inspection of the terms most highly associated with each individual topic. First, we propose a novel method for choosing which terms to present to a user to aid in the task of topic interpretation, in which we deﬁne the relevance of a term to a topic. Second, we present results from a user study that suggest that ranking terms purely by their probability under a topic is suboptimal for topic interpretation. Last, we describe LDAvis, our visualization system that allows users to ﬂexibly explore topic-term relationships using relevance to better understand a ﬁtted LDA model.},
	language = {en},
	urldate = {2019-07-26},
	booktitle = {Proceedings of the {Workshop} on {Interactive} {Language} {Learning}, {Visualization}, and {Interfaces}},
	publisher = {Association for Computational Linguistics},
	author = {Sievert, Carson and Shirley, Kenneth},
	year = {2014},
	pages = {63--70},
	file = {Sievert and Shirley - 2014 - LDAvis A method for visualizing and interpreting .pdf:/Users/YuhouZhou/Zotero/storage/WF56ZWBS/Sievert and Shirley - 2014 - LDAvis A method for visualizing and interpreting .pdf:application/pdf}
}

@inproceedings{roder_exploring_2015-1,
	address = {Shanghai, China},
	title = {Exploring the {Space} of {Topic} {Coherence} {Measures}},
	isbn = {978-1-4503-3317-7},
	url = {http://dl.acm.org/citation.cfm?doid=2684822.2685324},
	doi = {10.1145/2684822.2685324},
	abstract = {Quantifying the coherence of a set of statements is a long standing problem with many potential applications that has attracted researchers from diﬀerent sciences. The special case of measuring coherence of topics has been recently studied to remedy the problem that topic models give no guaranty on the interpretablity of their output. Several benchmark datasets were produced that record human judgements of the interpretability of topics. We are the ﬁrst to propose a framework that allows to construct existing word based coherence measures as well as new ones by combining elementary components. We conduct a systematic search of the space of coherence measures using all publicly available topic relevance data for the evaluation. Our results show that new combinations of components outperform existing measures with respect to correlation to human ratings. Finally, we outline how our results can be transferred to further applications in the context of text mining, information retrieval and the world wide web.},
	language = {en},
	urldate = {2019-08-21},
	booktitle = {Proceedings of the {Eighth} {ACM} {International} {Conference} on {Web} {Search} and {Data} {Mining} - {WSDM} '15},
	publisher = {ACM Press},
	author = {Röder, Michael and Both, Andreas and Hinneburg, Alexander},
	year = {2015},
	pages = {399--408},
	annote = {Introduce C\_V and C\_P},
	file = {Röder et al. - 2015 - Exploring the Space of Topic Coherence Measures.pdf:/Users/YuhouZhou/Zotero/storage/NFYTBXGJ/Röder et al. - 2015 - Exploring the Space of Topic Coherence Measures.pdf:application/pdf}
}

@article{aletras_evaluating_nodate,
	title = {Evaluating {Topic} {Coherence} {Using} {Distributional} {Semantics}},
	abstract = {This paper introduces distributional semantic similarity methods for automatically measuring the coherence of a set of words generated by a topic model. We construct a semantic space to represent each topic word by making use of Wikipedia as a reference corpus to identify context features and collect frequencies. Relatedness between topic words and context features is measured using variants of Pointwise Mutual Information (PMI). Topic coherence is determined by measuring the distance between these vectors computed using a variety of metrics. Evaluation on three data sets shows that the distributional-based measures outperform the state-of-the-art approach for this task.},
	language = {en},
	author = {Aletras, Nikolaos and Stevenson, Mark},
	pages = {9},
	annote = {Introduce C\_A and C\_NPMI
 
C\_NPMI is an enhanced version of the C\_UCI coherence using the normalized pointwise mutual information (NPMI) instead of the pointwise mutual information (PMI).},
	file = {Aletras and Stevenson - Evaluating Topic Coherence Using Distributional Se.pdf:/Users/YuhouZhou/Zotero/storage/PL8SP82Z/Aletras and Stevenson - Evaluating Topic Coherence Using Distributional Se.pdf:application/pdf}
}

@article{newman_automatic_nodate,
	title = {Automatic evaluation of topic coherence},
	abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on pointwise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
	language = {en},
	author = {Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy},
	pages = {9},
	annote = {Introduce C\_UCI},
	file = {Newman et al. - Automatic evaluation of topic coherence.pdf:/Users/YuhouZhou/Zotero/storage/IHBLBKF9/Newman et al. - Automatic evaluation of topic coherence.pdf:application/pdf}
}

@article{mimno_optimizing_nodate,
	title = {Optimizing semantic coherence in topic models},
	language = {en},
	author = {Mimno, David and Wallach, Hanna M and Talley, Edmund and Leenders, Miriam and McCallum, Andrew},
	pages = {11},
	annote = {Introduce C\_UMASS}
}

@incollection{hoffman_online_2010,
	title = {Online {Learning} for {Latent} {Dirichlet} {Allocation}},
	url = {http://papers.nips.cc/paper/3902-online-learning-for-latent-dirichlet-allocation.pdf},
	urldate = {2019-08-05},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 23},
	publisher = {Curran Associates, Inc.},
	author = {Hoffman, Matthew and Bach, Francis R. and Blei, David M.},
	editor = {Lafferty, J. D. and Williams, C. K. I. and Shawe-Taylor, J. and Zemel, R. S. and Culotta, A.},
	year = {2010},
	pages = {856--864},
	file = {NIPS Full Text PDF:/Users/YuhouZhou/Zotero/storage/JNKT89BU/Hoffman et al. - 2010 - Online Learning for Latent Dirichlet Allocation.pdf:application/pdf;NIPS Snapshot:/Users/YuhouZhou/Zotero/storage/N8EMPJAA/3902-online-learning-for-latent-dirichlet-allocation.html:text/html}
}

@article{hoffman_stochastic_nodate,
	title = {Stochastic {Variational} {Inference}},
	abstract = {We develop stochastic variational inference, a scalable algorithm for approximating posterior distributions. We develop this technique for a large class of probabilistic models and we demonstrate it with two probabilistic topic models, latent Dirichlet allocation and the hierarchical Dirichlet process topic model. Using stochastic variational inference, we analyze several large collections of documents: 300K articles from Nature, 1.8M articles from The New York Times, and 3.8M articles from Wikipedia. Stochastic inference can easily handle data sets of this size and outperforms traditional variational inference, which can only handle a smaller subset. (We also show that the Bayesian nonparametric topic model outperforms its parametric counterpart.) Stochastic variational inference lets us apply complex Bayesian models to massive data sets.},
	language = {en},
	author = {Hoffman, Matthew D},
	pages = {45},
	file = {Hoffman - Stochastic Variational Inference.pdf:/Users/YuhouZhou/Zotero/storage/957YTFX5/Hoffman - Stochastic Variational Inference.pdf:application/pdf}
}

@inproceedings{zhao_leveraging_2019,
	address = {Florence, Italy},
	title = {Leveraging {Meta} {Information} in {Short} {Text} {Aggregation}},
	url = {https://www.aclweb.org/anthology/P19-1396},
	abstract = {Short texts such as tweets often contain insufficient word co-occurrence information for training conventional topic models. To deal with the insufficiency, we propose a generative model that aggregates short texts into clusters by leveraging the associated meta information. Our model can generate more interpretable topics as well as document clusters. We develop an effective Gibbs sampling algorithm favoured by the fully local conjugacy in the model. Extensive experiments demonstrate that our model achieves better performance in terms of document clustering and topic coherence.},
	urldate = {2019-08-02},
	booktitle = {Proceedings of the 57th {Conference} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Zhao, He and Du, Lan and Liu, Guanfeng and Buntine, Wray},
	month = jul,
	year = {2019},
	pages = {4042--4049},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/Q4TTJ58V/Zhao et al. - 2019 - Leveraging Meta Information in Short Text Aggregat.pdf:application/pdf}
}

@article{newman_automatic_nodate-1,
	title = {Automatic evaluation of topic coherence},
	abstract = {This paper introduces the novel task of topic coherence evaluation, whereby a set of words, as generated by a topic model, is rated for coherence or interpretability. We apply a range of topic scoring models to the evaluation task, drawing on WordNet, Wikipedia and the Google search engine, and existing research on lexical similarity/relatedness. In comparison with human scores for a set of learned topics over two distinct datasets, we show a simple co-occurrence measure based on pointwise mutual information over Wikipedia data is able to achieve results for the task at or nearing the level of inter-annotator correlation, and that other Wikipedia-based lexical relatedness methods also achieve strong results. Google produces strong, if less consistent, results, while our results over WordNet are patchy at best.},
	language = {en},
	author = {Newman, David and Lau, Jey Han and Grieser, Karl and Baldwin, Timothy},
	pages = {9},
	file = {Newman et al. - Automatic evaluation of topic coherence.pdf:/Users/YuhouZhou/Zotero/storage/NW493HPD/Newman et al. - Automatic evaluation of topic coherence.pdf:application/pdf}
}

@inproceedings{hong_empirical_2010-1,
	address = {Washington D.C., District of Columbia},
	title = {Empirical study of topic modeling in {Twitter}},
	isbn = {978-1-4503-0217-3},
	url = {http://portal.acm.org/citation.cfm?doid=1964858.1964870},
	doi = {10.1145/1964858.1964870},
	abstract = {Social networks such as Facebook, LinkedIn, and Twitter have been a crucial source of information for a wide spectrum of users. In Twitter, popular information that is deemed important by the community propagates through the network. Studying the characteristics of content in the messages becomes important for a number of tasks, such as breaking news detection, personalized message recommendation, friends recommendation, sentiment analysis and others. While many researchers wish to use standard text mining tools to understand messages on Twitter, the restricted length of those messages prevents them from being employed to their full potential.},
	language = {en},
	urldate = {2019-08-23},
	booktitle = {Proceedings of the {First} {Workshop} on {Social} {Media} {Analytics} - {SOMA} '10},
	publisher = {ACM Press},
	author = {Hong, Liangjie and Davison, Brian D.},
	year = {2010},
	pages = {80--88}
}

@incollection{mikolov_distributed_2013,
	title = {Distributed {Representations} of {Words} and {Phrases} and their {Compositionality}},
	url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf},
	urldate = {2019-08-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 26},
	publisher = {Curran Associates, Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
	editor = {Burges, C. J. C. and Bottou, L. and Welling, M. and Ghahramani, Z. and Weinberger, K. Q.},
	year = {2013},
	pages = {3111--3119},
	file = {NIPS Full Text PDF:/Users/YuhouZhou/Zotero/storage/V9BHT565/Mikolov et al. - 2013 - Distributed Representations of Words and Phrases a.pdf:application/pdf}
}

@inproceedings{lau_sensitivity_2016,
	address = {San Diego, California},
	title = {The {Sensitivity} of {Topic} {Coherence} {Evaluation} to {Topic} {Cardinality}},
	url = {https://www.aclweb.org/anthology/N16-1057},
	doi = {10.18653/v1/N16-1057},
	urldate = {2019-08-30},
	booktitle = {Proceedings of the 2016 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Lau, Jey Han and Baldwin, Timothy},
	month = jun,
	year = {2016},
	pages = {483--487},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/UH563N9Q/Lau and Baldwin - 2016 - The Sensitivity of Topic Coherence Evaluation to T.pdf:application/pdf}
}

@incollection{chang_reading_2009,
	title = {Reading {Tea} {Leaves}: {How} {Humans} {Interpret} {Topic} {Models}},
	shorttitle = {Reading {Tea} {Leaves}},
	url = {http://papers.nips.cc/paper/3700-reading-tea-leaves-how-humans-interpret-topic-models.pdf},
	urldate = {2019-09-09},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 22},
	publisher = {Curran Associates, Inc.},
	author = {Chang, Jonathan and Gerrish, Sean and Wang, Chong and Boyd-graber, Jordan L. and Blei, David M.},
	editor = {Bengio, Y. and Schuurmans, D. and Lafferty, J. D. and Williams, C. K. I. and Culotta, A.},
	year = {2009},
	pages = {288--296},
	file = {NIPS Full Text PDF:/Users/YuhouZhou/Zotero/storage/5EARQ3CG/Chang et al. - 2009 - Reading Tea Leaves How Humans Interpret Topic Mod.pdf:application/pdf;NIPS Snapshot:/Users/YuhouZhou/Zotero/storage/ATFEND57/3700-reading-tea-leaves-how-humans-interpret-topic-models.html:text/html}
}

@inproceedings{choi_it_2015,
	address = {Beijing, China},
	title = {It {Depends}: {Dependency} {Parser} {Comparison} {Using} {A} {Web}-based {Evaluation} {Tool}},
	shorttitle = {It {Depends}},
	url = {https://www.aclweb.org/anthology/P15-1038},
	doi = {10.3115/v1/P15-1038},
	urldate = {2019-09-10},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Choi, Jinho D. and Tetreault, Joel and Stent, Amanda},
	month = jul,
	year = {2015},
	pages = {387--396},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/BK2MF53Q/Choi et al. - 2015 - It Depends Dependency Parser Comparison Using A W.pdf:application/pdf}
}

@inproceedings{choi_it_2015-1,
	address = {Beijing, China},
	title = {It {Depends}: {Dependency} {Parser} {Comparison} {Using} {A} {Web}-based {Evaluation} {Tool}},
	shorttitle = {It {Depends}},
	url = {https://www.aclweb.org/anthology/P15-1038},
	doi = {10.3115/v1/P15-1038},
	urldate = {2019-09-10},
	booktitle = {Proceedings of the 53rd {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 7th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Choi, Jinho D. and Tetreault, Joel and Stent, Amanda},
	month = jul,
	year = {2015},
	pages = {387--396},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/KQ25B2M9/Choi et al. - 2015 - It Depends Dependency Parser Comparison Using A W.pdf:application/pdf}
}

@article{bischof_summarizing_nodate,
	title = {Summarizing topical content with word frequency and exclusivity},
	abstract = {Recent work in text analysis commonly describes topics in terms of their most frequent words, but the exclusivity of words to topics is equally important for communicating content. We introduce Hierarchical Poisson Convolution (HPC), a model which infers regularized estimates of the diﬀerential use of words across topics as well as their frequency within topics. HPC uses known hierarchical structure on human-labeled topics to make focused comparisons of diﬀerential usage within each branch of the hierarchy of labels. We then infer a summary for each topic in terms of words that are both frequent and exclusive. We develop a parallelized Hamiltonian Monte Carlo sampler that allows for fast and scalable computation.},
	language = {en},
	author = {Bischof, Jonathan M and Airoldi, Edoardo M},
	pages = {8},
	file = {Bischof and Airoldi - Summarizing topical content with word frequency an.pdf:/Users/YuhouZhou/Zotero/storage/C8JQMZEB/Bischof and Airoldi - Summarizing topical content with word frequency an.pdf:application/pdf}
}

@article{rapaport_classification_2007,
	title = {Classification of microarray data using gene networks},
	volume = {8},
	issn = {1471-2105},
	url = {https://bmcbioinformatics.biomedcentral.com/articles/10.1186/1471-2105-8-35},
	doi = {10.1186/1471-2105-8-35},
	abstract = {Background: Microarrays have become extremely useful for analysing genetic phenomena, but establishing a relation between microarray analysis results (typically a list of genes) and their biological significance is often difficult. Currently, the standard approach is to map a posteriori the results onto gene networks in order to elucidate the functions perturbed at the level of pathways. However, integrating a priori knowledge of the gene networks could help in the statistical analysis of gene expression data and in their biological interpretation.
Results: We propose a method to integrate a priori the knowledge of a gene network in the analysis of gene expression data. The approach is based on the spectral decomposition of gene expression profiles with respect to the eigenfunctions of the graph, resulting in an attenuation of the high-frequency components of the expression profiles with respect to the topology of the graph. We show how to derive unsupervised and supervised classification algorithms of expression profiles, resulting in classifiers with biological relevance. We illustrate the method with the analysis of a set of expression profiles from irradiated and non-irradiated yeast strains.
Conclusion: Including a priori knowledge of a gene network for the analysis of gene expression data leads to good classification performance and improved interpretability of the results.},
	language = {en},
	number = {1},
	urldate = {2019-10-22},
	journal = {BMC Bioinformatics},
	author = {Rapaport, Franck and Zinovyev, Andrei and Dutreix, Marie and Barillot, Emmanuel and Vert, Jean-Philippe},
	month = dec,
	year = {2007},
	annote = {Spectral decomposition for a metabolic network (not regulatory network), but the method can be applied to different gene networks.
However, this paper uses undirected graph.},
	file = {Rapaport et al. - 2007 - Classification of microarray data using gene netwo.pdf:/Users/YuhouZhou/Zotero/storage/4EUKIC4T/Rapaport et al. - 2007 - Classification of microarray data using gene netwo.pdf:application/pdf}
}

@article{barabasi_network_2004,
	title = {Network biology: understanding the cell's functional organization},
	volume = {5},
	issn = {1471-0056, 1471-0064},
	shorttitle = {Network biology},
	url = {http://www.nature.com/articles/nrg1272},
	doi = {10.1038/nrg1272},
	language = {en},
	number = {2},
	urldate = {2019-10-24},
	journal = {Nature Reviews Genetics},
	author = {Barabási, Albert-László and Oltvai, Zoltán N.},
	month = feb,
	year = {2004},
	pages = {101--113},
	file = {Barabási and Oltvai - 2004 - Network biology understanding the cell's function.pdf:/Users/YuhouZhou/Zotero/storage/ZIQJSZ46/Barabási and Oltvai - 2004 - Network biology understanding the cell's function.pdf:application/pdf}
}

@article{clough_gene_2016,
	title = {The {Gene} {Expression} {Omnibus} database},
	volume = {1418},
	issn = {1064-3745},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4944384/},
	doi = {10.1007/978-1-4939-3578-9_5},
	abstract = {The Gene Expression Omnibus (GEO) database is an international public repository that archives and freely distributes high-throughput gene expression and other functional genomics data sets. Created in 2000 as a worldwide resource for gene expression studies, GEO has evolved with rapidly changing technologies and now accepts high-throughput data for many other data applications, including those that examine genome methylation, chromatin structure, and genome–protein interactions. GEO supports community-derived reporting standards that specify provision of several critical study elements including raw data, processed data, and descriptive metadata. The database not only provides access to data for tens of thousands of studies, but also offers various Web-based tools and strategies that enable users to locate data relevant to their specific interests, as well as to visualize and analyze the data. This chapter includes detailed descriptions of methods to query and download GEO data and use the analysis and visualization tools. The GEO homepage is at http://www.ncbi.nlm.nih.gov/geo/.},
	urldate = {2019-10-27},
	journal = {Methods in molecular biology (Clifton, N.J.)},
	author = {Clough, Emily and Barrett, Tanya},
	year = {2016},
	pmid = {27008011},
	pmcid = {PMC4944384},
	pages = {93--110},
	file = {PubMed Central Full Text PDF:/Users/YuhouZhou/Zotero/storage/MNGZMBS7/Clough and Barrett - 2016 - The Gene Expression Omnibus database.pdf:application/pdf}
}

@article{gama-castro_regulondb_2016,
	title = {{RegulonDB} version 9.0: high-level integration of gene regulation, coexpression, motif clustering and beyond},
	volume = {44},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{RegulonDB} version 9.0},
	url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkv1156},
	doi = {10.1093/nar/gkv1156},
	language = {en},
	number = {D1},
	urldate = {2019-10-27},
	journal = {Nucleic Acids Research},
	author = {Gama-Castro, Socorro and Salgado, Heladia and Santos-Zavaleta, Alberto and Ledezma-Tejeida, Daniela and Muñiz-Rascado, Luis and García-Sotelo, Jair Santiago and Alquicira-Hernández, Kevin and Martínez-Flores, Irma and Pannier, Lucia and Castro-Mondragón, Jaime Abraham and Medina-Rivera, Alejandra and Solano-Lira, Hilda and Bonavides-Martínez, César and Pérez-Rueda, Ernesto and Alquicira-Hernández, Shirley and Porrón-Sotelo, Liliana and López-Fuentes, Alejandra and Hernández-Koutoucheva, Anastasia and Moral-Chávez, Víctor Del and Rinaldi, Fabio and Collado-Vides, Julio},
	month = jan,
	year = {2016},
	pages = {D133--D143},
	file = {Gama-Castro et al. - 2016 - RegulonDB version 9.0 high-level integration of g.pdf:/Users/YuhouZhou/Zotero/storage/RAX2WEKT/Gama-Castro et al. - 2016 - RegulonDB version 9.0 high-level integration of g.pdf:application/pdf}
}

@misc{noauthor_nicholas_nodate,
	title = {Nicholas {Luscombe}: {Visualizing} {Gene} {Regulatory} {Networks} - {YouTube}},
	url = {https://www.youtube.com/watch?v=bzw2GMeSZuw},
	urldate = {2019-10-28},
	file = {Nicholas Luscombe\: Visualizing Gene Regulatory Networks - YouTube:/Users/YuhouZhou/Zotero/storage/EQJBHQCG/watch.html:text/html}
}

@article{santos-zavaleta_regulondb_2019,
	title = {{RegulonDB} v 10.5: tackling challenges to unify classic and high throughput knowledge of gene regulation in {E}. coli {K}-12},
	volume = {47},
	issn = {0305-1048},
	shorttitle = {{RegulonDB} v 10.5},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6324031/},
	doi = {10.1093/nar/gky1077},
	abstract = {RegulonDB, first published 20 years ago, is a comprehensive electronic resource about regulation of transcription initiation of Escherichia coli K-12 with decades of knowledge from classic molecular biology experiments, and recently also from high-throughput genomic methodologies. We curated the literature to keep RegulonDB up to date, and initiated curation of ChIP and gSELEX experiments. We estimate that current knowledge describes between 10\% and 30\% of the expected total number of transcription factor- gene regulatory interactions in E. coli. RegulonDB provides datasets for interactions for which there is no evidence that they affect expression, as well as expression datasets. We developed a proof of concept pipeline to merge binding and expression evidence to identify regulatory interactions. These datasets can be visualized in the RegulonDB JBrowse. We developed the Microbial Conditions Ontology with a controlled vocabulary for the minimal properties to reproduce an experiment, which contributes to integrate data from high throughput and classic literature. At a higher level of integration, we report Genetic Sensory-Response Units for 200 transcription factors, including their regulation at the metabolic level, and include summaries for 70 of them. Finally, we summarize our research with Natural language processing strategies to enhance our biocuration work.},
	number = {Database issue},
	urldate = {2019-11-01},
	journal = {Nucleic Acids Research},
	author = {Santos-Zavaleta, Alberto and Salgado, Heladia and Gama-Castro, Socorro and Sánchez-Pérez, Mishael and Gómez-Romero, Laura and Ledezma-Tejeida, Daniela and García-Sotelo, Jair Santiago and Alquicira-Hernández, Kevin and Muñiz-Rascado, Luis José and Peña-Loredo, Pablo and Ishida-Gutiérrez, Cecilia and Velázquez-Ramírez, David A and Del Moral-Chávez, Víctor and Bonavides-Martínez, César and Méndez-Cruz, Carlos-Francisco and Galagan, James and Collado-Vides, Julio},
	month = jan,
	year = {2019},
	pmid = {30395280},
	pmcid = {PMC6324031},
	pages = {D212--D220},
	file = {PubMed Central Full Text PDF:/Users/YuhouZhou/Zotero/storage/BYWQW5Q5/Santos-Zavaleta et al. - 2019 - RegulonDB v 10.5 tackling challenges to unify cla.pdf:application/pdf}
}

@article{salgado_using_2018,
	title = {Using {RegulonDB}, the {Escherichia} coli {K}-12 gene regulatory transcriptional network database},
	volume = {61},
	issn = {1934-3396},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6060643/},
	doi = {10.1002/cpbi.43},
	abstract = {In RegulonDB we have been gathering knowledge by manual curation from original scientific literature on the regulation of transcription initiation and the genome organization in transcription units of the Escherichia coli K-12 genome for over 25 years. This paper describes six basic protocols that can serve as a guiding introduction to the main content of the current version (v9.4) of this electronic resource. These include the general navigation as well as searching for specific objects such as genes, gene products, transcription units, promoters, transcription factors, coexpression, and Genetic sensory response units or GENSOR Units. Following the format of these protocols the user will find an initial introduction to the concepts pertinent to the protocol, what is the content when performing the given navigation, as well as the necessary resources. This easy to follow protocol shall help anyone interested to quickly see all what is currently offered in RegulonDB, including position weight matrices of transcription factors, coexpression values based on published microarrays, as well as the GENSOR Units unique to RegulonDB that offer regulatory mechanisms in the context of their signals and metabolic consequences.},
	number = {1},
	urldate = {2019-11-01},
	journal = {Current protocols in bioinformatics},
	author = {Salgado, Heladia and Martínez-Flores, Irma and Bustamante, Víctor H. and Alquicira-Hernández, Kevin and García-Sotelo, Jair S. and García-Alonso, Delfino and Collado-Vides, Julio},
	month = mar,
	year = {2018},
	pmid = {30040192},
	pmcid = {PMC6060643},
	pages = {1.32.1--1.32.30},
	file = {PubMed Central Full Text PDF:/Users/YuhouZhou/Zotero/storage/GQWVB627/Salgado et al. - 2018 - Using RegulonDB, the Escherichia coli K-12 gene re.pdf:application/pdf}
}

@article{featherstone_wrestling_2002,
	title = {Wrestling with pleiotropy: {Genomic} and topological analysis of the yeast gene expression network},
	volume = {24},
	issn = {1521-1878},
	shorttitle = {Wrestling with pleiotropy},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/bies.10054},
	doi = {10.1002/bies.10054},
	abstract = {The vast majority ({\textgreater} 95\%) of single-gene mutations in yeast affect not only the expression of the mutant gene, but also the expression of many other genes. These data suggest the presence of a previously uncharacterized ‘gene expression network’—a set of interactions between genes which dictate gene expression in the native cell environment. Here, we quantitatively analyze the gene expression network revealed by microarray expression data from 273 different yeast gene deletion mutants.(1) We find that gene expression interactions form a robust, error-tolerant ‘scale-free’ network, similar to metabolic pathways(2) and artificial networks such as power grids and the internet.(3–5) Because the connectivity between genes in the gene expression network is unevenly distributed, a scale-free organization helps make organisms resistant to the deleterious effects of mutation, and is thus highly adaptive. The existence of a gene expression network poses practical considerations for the study of gene function, since most mutant phenotypes are the result of changes in the expression of many genes. Using principles of scale-free network topology, we propose that fragmenting the gene expression network via ‘genome-engineering’ may be a viable and practical approach to isolating gene function. BioEssays 24:267–274, 2002. © 2002 Wiley Periodicals, Inc.; DOI 10.1002/bies.10054},
	language = {en},
	number = {3},
	urldate = {2019-11-03},
	journal = {BioEssays},
	author = {Featherstone, David E. and Broadie, Kendal},
	year = {2002},
	pages = {267--274},
	file = {Full Text PDF:/Users/YuhouZhou/Zotero/storage/W4NLXYWU/Featherstone and Broadie - 2002 - Wrestling with pleiotropy Genomic and topological.pdf:application/pdf;Snapshot:/Users/YuhouZhou/Zotero/storage/GS9YRIKQ/bies.html:text/html}
}

@article{agrawal_extreme_2002,
	title = {Extreme {Self}-{Organization} in {Networks} {Constructed} from {Gene} {Expression} {Data}},
	volume = {89},
	issn = {0031-9007, 1079-7114},
	url = {https://link.aps.org/doi/10.1103/PhysRevLett.89.268702},
	doi = {10.1103/PhysRevLett.89.268702},
	language = {en},
	number = {26},
	urldate = {2019-11-03},
	journal = {Physical Review Letters},
	author = {Agrawal, Himanshu},
	month = dec,
	year = {2002},
	file = {Agrawal - 2002 - Extreme Self-Organization in Networks Constructed .pdf:/Users/YuhouZhou/Zotero/storage/NVIPQH6D/Agrawal - 2002 - Extreme Self-Organization in Networks Constructed .pdf:application/pdf}
}

@article{davidson_gene_2005,
	title = {Gene regulatory networks},
	volume = {102},
	issn = {0027-8424},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC556010/},
	doi = {10.1073/pnas.0502024102},
	number = {14},
	urldate = {2019-11-03},
	journal = {Proceedings of the National Academy of Sciences of the United States of America},
	author = {Davidson, Eric and Levin, Michael},
	month = apr,
	year = {2005},
	pmid = {15809445},
	pmcid = {PMC556010},
	pages = {4935},
	file = {PubMed Central Full Text PDF:/Users/YuhouZhou/Zotero/storage/KY7XZP3W/Davidson and Levin - 2005 - Gene regulatory networks.pdf:application/pdf}
}

@article{dinkla_compressed_2012,
	title = {Compressed {Adjacency} {Matrices}: {Untangling} {Gene} {Regulatory} {Networks}},
	volume = {18},
	issn = {1077-2626, 1941-0506, 2160-9306},
	shorttitle = {Compressed {Adjacency} {Matrices}},
	doi = {10.1109/TVCG.2012.208},
	abstract = {We present a novel technique-Compressed Adjacency Matrices-for visualizing gene regulatory networks. These directed networks have strong structural characteristics: out-degrees with a scale-free distribution, in-degrees bound by a low maximum, and few and small cycles. Standard visualization techniques, such as node-link diagrams and adjacency matrices, are impeded by these network characteristics. The scale-free distribution of out-degrees causes a high number of intersecting edges in node-link diagrams. Adjacency matrices become space-inefficient due to the low in-degrees and the resulting sparse network. Compressed adjacency matrices, however, exploit these structural characteristics. By cutting open and rearranging an adjacency matrix, we achieve a compact and neatly-arranged visualization. Compressed adjacency matrices allow for easy detection of subnetworks with a specific structure, so-called motifs, which provide important knowledge about gene regulatory networks to domain experts. We summarize motifs commonly referred to in the literature, and relate them to network analysis tasks common to the visualization domain. We show that a user can easily find the important motifs in compressed adjacency matrices, and that this is hard in standard adjacency matrix and node-link diagrams. We also demonstrate that interaction techniques for standard adjacency matrices can be used for our compressed variant. These techniques include rearrangement clustering, highlighting, and filtering.},
	number = {12},
	journal = {IEEE Transactions on Visualization and Computer Graphics},
	author = {Dinkla, Kasper and Westenberg, Michel A. and van Wijk, Jarke J.},
	month = dec,
	year = {2012},
	keywords = {adjacency matrix, Algorithms, biology computing, Bismuth, compressed adjacency matrices, Computer aided manufacturing, Computer Graphics, data visualisation, directed networks, gene regulation, gene regulatory networks, Gene Regulatory Networks, genetics, Image Processing, Computer-Assisted, Layout, matrix algebra, motifs, neatly-arranged visualization, Network, network characteristics, network theory (graphs), node-link diagrams, Proteins, rearrangement clustering, scale-free, scale-free distribution, Sparse matrices, sparse network, standard adjacency matrix, standard visualization, Standards, structural characteristics, Visualization, visualization domain},
	pages = {2457--2466},
	file = {IEEE Xplore Abstract Record:/Users/YuhouZhou/Zotero/storage/GLFKHQCM/6327251.html:text/html;IEEE Xplore Full Text PDF:/Users/YuhouZhou/Zotero/storage/TH8BEMHB/Dinkla et al. - 2012 - Compressed Adjacency Matrices Untangling Gene Reg.pdf:application/pdf}
}

@article{akman_spectral_2018,
	title = {Spectral {Functional}-{Digraph} {Theory}, {Stability}, and {Entropy} for {Gene} {Regulatory} {Networks}},
	volume = {4},
	issn = {2297-4687},
	url = {https://www.frontiersin.org/article/10.3389/fams.2018.00028/full},
	doi = {10.3389/fams.2018.00028},
	abstract = {Spectral graph theory is an indispensable tool in the rich interdisciplinary ﬁeld of network science, which includes as objects ordinary abstract graphs as well as directed graphs such as the Internet, semantic networks, electrical circuits, and gene regulatory networks (GRN). However, its contributions sometimes get lost in the code, and network theory occasionally becomes overwhelmed with problems speciﬁc to undirected graphs. In this paper, we will study functional digraphs, calculate the eigenvalues and eigenvectors of their adjacency matrices, describe how to compute their automorphism groups, and deﬁne a notion of entropy in terms of their symmetries. We will then introduce gene regulatory networks (GRNs) from scratch, and consider their phase spaces, which are functional digraphs describing the deterministic progression of the overall state of a GRN. Finally, we will redeﬁne the stability of a GRN and assert that it is closely related to the entropy of its phase space.},
	language = {en},
	urldate = {2019-11-03},
	journal = {Frontiers in Applied Mathematics and Statistics},
	author = {Akman, Devin and Akman, Füsun},
	month = jul,
	year = {2018},
	file = {Akman and Akman - 2018 - Spectral Functional-Digraph Theory, Stability, and.pdf:/Users/YuhouZhou/Zotero/storage/ZTHARWI3/Akman and Akman - 2018 - Spectral Functional-Digraph Theory, Stability, and.pdf:application/pdf}
}

@misc{noauthor_spectral_nodate,
	title = {Spectral graph theory - {Wikiwand}},
	url = {https://www.wikiwand.com/en/Spectral_graph_theory},
	urldate = {2019-11-03},
	file = {Spectral graph theory - Wikiwand:/Users/YuhouZhou/Zotero/storage/BBR93JCA/Spectral_graph_theory.html:text/html}
}

@misc{noauthor_spectral_nodate-1,
	title = {Spectral {Graph} {Theory} - {Fall} 2015},
	url = {http://www.cs.yale.edu/homes/spielman/561/},
	urldate = {2019-11-03},
	file = {Spectral Graph Theory - Fall 2015:/Users/YuhouZhou/Zotero/storage/CK9BNYZ9/561.html:text/html}
}

@article{spielman_spectral_nodate,
	title = {Spectral {Graph} {Theory} and its {Applications}},
	language = {en},
	author = {Spielman, Daniel A},
	pages = {75},
	file = {Spielman - Spectral Graph Theory and its Applications.pdf:/Users/YuhouZhou/Zotero/storage/V5IQCSRA/Spielman - Spectral Graph Theory and its Applications.pdf:application/pdf}
}

@article{chung_lectures_nodate,
	title = {Lectures on {Spectral} {Graph} {Theory}},
	language = {en},
	author = {Chung, Fan R K},
	pages = {25},
	file = {Chung - Lectures on Spectral Graph Theory.pdf:/Users/YuhouZhou/Zotero/storage/P87G3ZZF/Chung - Lectures on Spectral Graph Theory.pdf:application/pdf}
}

@article{roughgarden_cs168:_nodate,
	title = {{CS168}: {The} {Modern} {Algorithmic} {Toolbox} {Lectures} \#11: {Spectral} {Graph} {Theory}, {I}},
	language = {en},
	author = {Roughgarden, Tim and Valiant, Gregory},
	pages = {6},
	file = {Roughgarden and Valiant - CS168 The Modern Algorithmic Toolbox Lectures #11.pdf:/Users/YuhouZhou/Zotero/storage/E93QSN53/Roughgarden and Valiant - CS168 The Modern Algorithmic Toolbox Lectures #11.pdf:application/pdf}
}

@article{roughgarden_cs168:_nodate-1,
	title = {{CS168}: {The} {Modern} {Algorithmic} {Toolbox} \#12: {Spectral} {Graph} {Theory}, {Part} 2},
	language = {en},
	author = {Roughgarden, Tim and Valiant, Gregory},
	pages = {4},
	file = {Roughgarden and Valiant - CS168 The Modern Algorithmic Toolbox #12 Spectra.pdf:/Users/YuhouZhou/Zotero/storage/UL7UQ7KB/Roughgarden and Valiant - CS168 The Modern Algorithmic Toolbox #12 Spectra.pdf:application/pdf}
}

@article{spielman_adjacency_2012,
	title = {The {Adjacency} {Matrix} and {The} nth {Eigenvalue}},
	language = {en},
	author = {Spielman, Daniel A},
	year = {2012},
	pages = {8},
	file = {Spielman - 2012 - The Adjacency Matrix and The nth Eigenvalue.pdf:/Users/YuhouZhou/Zotero/storage/K7YPFDG8/Spielman - 2012 - The Adjacency Matrix and The nth Eigenvalue.pdf:application/pdf}
}

@incollection{hahn_applications_1997,
	address = {Dordrecht},
	title = {Some applications of {Laplace} eigenvalues of graphs},
	isbn = {978-90-481-4885-1 978-94-015-8937-6},
	url = {http://link.springer.com/10.1007/978-94-015-8937-6_6},
	abstract = {In the last decade important relations between Laplace eigenvalues and eigenvectors of graphs and several other graph parameters were discovered. In these notes we present some of these results and discuss their consequences. Attention is given to the partition and the isoperimetric properties of graphs, the max-cut problem and its relation to semideﬁnite programming, rapid mixing of Markov chains, and to extensions of the results to inﬁnite graphs.},
	language = {en},
	urldate = {2019-11-04},
	booktitle = {Graph {Symmetry}},
	publisher = {Springer Netherlands},
	author = {Mohar, Bojan},
	editor = {Hahn, Geňa and Sabidussi, Gert},
	year = {1997},
	doi = {10.1007/978-94-015-8937-6_6},
	pages = {225--275},
	file = {Mohar - 1997 - Some applications of Laplace eigenvalues of graphs.pdf:/Users/YuhouZhou/Zotero/storage/LUY5Y9V8/Mohar - 1997 - Some applications of Laplace eigenvalues of graphs.pdf:application/pdf}
}

@book{chung_spectral_1997,
	address = {Providence, R.I},
	series = {Regional conference series in mathematics},
	title = {Spectral graph theory},
	isbn = {978-0-8218-0315-8},
	language = {en},
	number = {no. 92},
	publisher = {Published for the Conference Board of the mathematical sciences by the American Mathematical Society},
	author = {Chung, Fan R. K.},
	year = {1997},
	keywords = {Congresses, Eigenvalues, Graph theory},
	file = {Chung - 1997 - Spectral graph theory.pdf:/Users/YuhouZhou/Zotero/storage/CAWY9I39/Chung - 1997 - Spectral graph theory.pdf:application/pdf}
}

@book{nica_brief_2018,
	address = {Zuerich, Switzerland},
	title = {A {Brief} {Introduction} to {Spectral} {Graph} {Theory}},
	isbn = {978-3-03719-188-0},
	url = {http://www.ems-ph.org/doi/10.4171/188},
	language = {en},
	urldate = {2019-11-04},
	publisher = {European Mathematical Society Publishing House},
	author = {Nica, Bogdan},
	month = may,
	year = {2018},
	doi = {10.4171/188},
	file = {Nica - 2018 - A Brief Introduction to Spectral Graph Theory.pdf:/Users/YuhouZhou/Zotero/storage/PGS6HKD6/Nica - 2018 - A Brief Introduction to Spectral Graph Theory.pdf:application/pdf}
}

@misc{noauthor_doi:10.1016/j.camwa.2004.05.005_nodate,
	title = {doi:10.1016/j.camwa.2004.05.005 {\textbar} {Elsevier} {Enhanced} {Reader}},
	shorttitle = {doi},
	url = {https://reader.elsevier.com/reader/sd/pii/S0898122104003074?token=6694A1C86E234B19BA9A262011AC160E8151724D12548C6C9AECFD8D0F5E0EDC283C5F97C233DA1E1DCE265E0265A7D6},
	language = {en},
	urldate = {2019-11-04},
	doi = {10.1016/j.camwa.2004.05.005},
	file = {Full Text:/Users/YuhouZhou/Zotero/storage/6JKVUMIQ/doi10.1016j.camwa.2004.05.005  Elsevier Enhance.pdf:application/pdf;Snapshot:/Users/YuhouZhou/Zotero/storage/2IBK3SYM/S0898122104003074.html:text/html}
}

@article{nakao_turing_2010,
	title = {Turing patterns in network-organized activator–inhibitor systems},
	volume = {6},
	issn = {1745-2473, 1745-2481},
	url = {http://www.nature.com/articles/nphys1651},
	doi = {10.1038/nphys1651},
	language = {en},
	number = {7},
	urldate = {2019-11-06},
	journal = {Nature Physics},
	author = {Nakao, Hiroya and Mikhailov, Alexander S.},
	month = jul,
	year = {2010},
	pages = {544--550},
	file = {Nakao and Mikhailov - 2010 - Turing patterns in network-organized activator–inh.pdf:/Users/YuhouZhou/Zotero/storage/MXFX5HAT/Nakao and Mikhailov - 2010 - Turing patterns in network-organized activator–inh.pdf:application/pdf}
}

@article{higham_black-scholes_2004,
	title = {Black-{Scholes} for scientific computing students},
	volume = {6},
	issn = {1558-366X},
	doi = {10.1109/MCSE.2004.62},
	abstract = {Mathematical finance provides a modern, attractive source of examples and case studies for scientific computing classes. In this installment of education, the author shows how educators can use the Nobel Prize-winning Black-Scholes option valuation theory to motivate exercises in Monte Carlo simulation, matrix computation, and numerical methods for partial differential equations (PDEs).},
	number = {6},
	journal = {Computing in Science Engineering},
	author = {Higham, D.J.},
	month = nov,
	year = {2004},
	keywords = {matrix algebra, Black-Scholes option valuation theory, Books, Computer aided software engineering, Contracts, Cost accounting, econophysics, education, finance, Finance, Histograms, mathematical finance, Mathematical model, mathematics computing, matrix computation, Monte Carlo methods, Monte Carlo simulation, numerical methods, partial differential equations, Predictive models, Random variables, scientific computing, Scientific computing, scientific computing classes, scientific computing students},
	pages = {72--79},
	file = {IEEE Xplore Abstract Record:/Users/YuhouZhou/Zotero/storage/M58MX3IM/1353184.html:text/html;IEEE Xplore Full Text PDF:/Users/YuhouZhou/Zotero/storage/ERVEMXQY/Higham - 2004 - Black-Scholes for scientific computing students.pdf:application/pdf}
}

@article{kanehisa_kegg:_2000,
	title = {{KEGG}: {Kyoto} {Encyclopedia} of {Genes} and {Genomes}},
	volume = {28},
	issn = {0305-1048},
	shorttitle = {{KEGG}},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC102409/},
	abstract = {KEGG (Kyoto Encyclopedia of Genes and Genomes) is a knowledge base for systematic analysis of gene functions, linking genomic information with higher order functional information. The genomic information is stored in the GENES database, which is a collection of gene catalogs for all the completely sequenced genomes and some partial genomes with up-to-date annotation of gene functions. The higher order functional information is stored in the PATHWAY database, which contains graphical representations of cellular processes, such as metabolism, membrane transport, signal transduction and cell cycle. The PATHWAY database is supplemented by a set of ortholog group tables for the information about conserved subpathways (pathway motifs), which are often encoded by positionally coupled genes on the chromosome and which are especially useful in predicting gene functions. A third database in KEGG is LIGAND for the information about chemical compounds, enzyme molecules and enzymatic reactions. KEGG provides Java graphics tools for browsing genome maps, comparing two genome maps and manipulating expression maps, as well as computational tools for sequence comparison, graph comparison and path computation. The KEGG databases are daily updated and made freely available (http://www.genome.ad.jp/kegg/ ).},
	number = {1},
	urldate = {2019-12-02},
	journal = {Nucleic Acids Research},
	author = {Kanehisa, Minoru and Goto, Susumu},
	month = jan,
	year = {2000},
	pmid = {10592173},
	pmcid = {PMC102409},
	pages = {27--30},
	file = {PubMed Central Full Text PDF:/Users/YuhouZhou/Zotero/storage/6IBIBQU9/Kanehisa and Goto - 2000 - KEGG Kyoto Encyclopedia of Genes and Genomes.pdf:application/pdf}
}

@article{kanehisa_new_2019,
	title = {New approach for understanding genome variations in {KEGG}},
	volume = {47},
	issn = {0305-1048},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6324070/},
	doi = {10.1093/nar/gky962},
	abstract = {KEGG (Kyoto Encyclopedia of Genes and Genomes; https://www.kegg.jp/ or https://www.genome.jp/kegg/) is a reference knowledge base for biological interpretation of genome sequences and other high-throughput data. It is an integrated database consisting of three generic categories of systems information, genomic information and chemical information, and an additional human-specific category of health information. KEGG pathway maps, BRITE hierarchies and KEGG modules have been developed as generic molecular networks with KEGG Orthology nodes of functional orthologs so that KEGG pathway mapping and other procedures can be applied to any cellular organism. Unfortunately, however, this generic approach was inadequate for knowledge representation in the health information category, where variations of human genomes, especially disease-related variations, had to be considered. Thus, we have introduced a new approach where human gene variants are explicitly incorporated into what we call ‘network variants’ in the recently released KEGG NETWORK database. This allows accumulation of knowledge about disease-related perturbed molecular networks caused not only by gene variants, but also by viruses and other pathogens, environmental factors and drugs. We expect that KEGG NETWORK will become another reference knowledge base for the basic understanding of disease mechanisms and practical use in clinical sequencing and drug development.},
	number = {Database issue},
	urldate = {2019-12-02},
	journal = {Nucleic Acids Research},
	author = {Kanehisa, Minoru and Sato, Yoko and Furumichi, Miho and Morishima, Kanae and Tanabe, Mao},
	month = jan,
	year = {2019},
	pmid = {30321428},
	pmcid = {PMC6324070},
	pages = {D590--D595},
	file = {PubMed Central Full Text PDF:/Users/YuhouZhou/Zotero/storage/BBLNRGRU/Kanehisa et al. - 2019 - New approach for understanding genome variations i.pdf:application/pdf}
}

@misc{noauthor_drawing_nodate,
	title = {Drawing {Kegg} {Pathways}},
	url = {https://www.biostars.org/p/65289/},
	urldate = {2019-12-03},
	file = {Drawing Kegg Pathways:/Users/YuhouZhou/Zotero/storage/Y5GLZCX2/65289.html:text/html}
}

@article{duarte_reconstruction_2004,
	title = {Reconstruction and {Validation} of {Saccharomyces} cerevisiae {iND750}, a {Fully} {Compartmentalized} {Genome}-{Scale} {Metabolic} {Model}},
	volume = {14},
	issn = {1088-9051},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC442145/},
	doi = {10.1101/gr.2250904},
	abstract = {A fully compartmentalized genome-scale metabolic model of Saccharomyces cerevisiae that accounts for 750 genes and their associated transcripts, proteins, and reactions has been reconstructed and validated. All of the 1149 reactions included in this in silico model are both elementally and charge balanced and have been assigned to one of eight cellular locations (extracellular space, cytosol, mitochondrion, peroxisome, nucleus, endoplasmic reticulum, Golgi apparatus, or vacuole). When in silico predictions of 4154 growth phenotypes were compared to two published large-scale gene deletion studies, an 83\% agreement was found between iND750's predictions and the experimental studies. Analysis of the failure modes showed that false predictions were primarily caused by iND750's limited inclusion of cellular processes outside of metabolism. This study systematically identified inconsistencies in our knowledge of yeast metabolism that require specific further experimental investigation.},
	number = {7},
	urldate = {2020-01-09},
	journal = {Genome Research},
	author = {Duarte, Natalie C. and Herrgård, Markus J. and Palsson, Bernhard Ø.},
	month = jul,
	year = {2004},
	pmid = {15197165},
	pmcid = {PMC442145},
	pages = {1298--1309},
	file = {PubMed Central Full Text PDF:/Users/YuhouZhou/Zotero/storage/BIZK6ND8/Duarte et al. - 2004 - Reconstruction and Validation of Saccharomyces cer.pdf:application/pdf}
}

@article{norsigian_bigg_2019,
	title = {{BiGG} {Models} 2020: multi-strain genome-scale models and expansion across the phylogenetic tree},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{BiGG} {Models} 2020},
	url = {https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkz1054/5614178},
	doi = {10.1093/nar/gkz1054},
	abstract = {The BiGG Models knowledge base (http://bigg.ucsd. edu) is a centralized repository for high-quality genome-scale metabolic models. For the past 12 years, the website has allowed users to browse and search metabolic models. Within this update, we detail new content and features in the repository, continuing the original effort to connect each model to genome annotations and external databases as well as standardization of reactions and metabolites. We describe the addition of 31 new models that expand the portion of the phylogenetic tree covered by BiGG Models. We also describe new functionality for hosting multi-strain models, which have proven to be insightful in a variety of studies centered on comparisons of related strains. Finally, the models in the knowledge base have been benchmarked using Memote, a new community-developed validator for genome-scale models to demonstrate the improving quality and transparency of model content in BiGG Models.},
	language = {en},
	urldate = {2020-01-15},
	journal = {Nucleic Acids Research},
	author = {Norsigian, Charles J and Pusarla, Neha and McConn, John Luke and Yurkovich, James T and Dräger, Andreas and Palsson, Bernhard O and King, Zachary},
	month = nov,
	year = {2019},
	file = {Norsigian et al. - 2019 - BiGG Models 2020 multi-strain genome-scale models.pdf:/Users/YuhouZhou/Zotero/storage/NHF74465/Norsigian et al. - 2019 - BiGG Models 2020 multi-strain genome-scale models.pdf:application/pdf}
}

@article{norsigian_bigg_2019-1,
	title = {{BiGG} {Models} 2020: multi-strain genome-scale models and expansion across the phylogenetic tree},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{BiGG} {Models} 2020},
	url = {https://academic.oup.com/nar/advance-article/doi/10.1093/nar/gkz1054/5614178},
	doi = {10.1093/nar/gkz1054},
	abstract = {The BiGG Models knowledge base (http://bigg.ucsd. edu) is a centralized repository for high-quality genome-scale metabolic models. For the past 12 years, the website has allowed users to browse and search metabolic models. Within this update, we detail new content and features in the repository, continuing the original effort to connect each model to genome annotations and external databases as well as standardization of reactions and metabolites. We describe the addition of 31 new models that expand the portion of the phylogenetic tree covered by BiGG Models. We also describe new functionality for hosting multi-strain models, which have proven to be insightful in a variety of studies centered on comparisons of related strains. Finally, the models in the knowledge base have been benchmarked using Memote, a new community-developed validator for genome-scale models to demonstrate the improving quality and transparency of model content in BiGG Models.},
	language = {en},
	urldate = {2020-01-15},
	journal = {Nucleic Acids Research},
	author = {Norsigian, Charles J and Pusarla, Neha and McConn, John Luke and Yurkovich, James T and Dräger, Andreas and Palsson, Bernhard O and King, Zachary},
	month = nov,
	year = {2019},
	file = {Norsigian et al. - 2019 - BiGG Models 2020 multi-strain genome-scale models.pdf:/Users/YuhouZhou/Zotero/storage/4N8M89IL/Norsigian et al. - 2019 - BiGG Models 2020 multi-strain genome-scale models.pdf:application/pdf}
}

@misc{noauthor_bigg_nodate,
	title = {{BiGG} {Models} {ID} {Specification} and {Guidelines} · {SBRG}/bigg\_models {Wiki}},
	url = {https://github.com/SBRG/bigg_models/wiki/BiGG-Models-ID-Specification-and-Guidelines},
	urldate = {2020-01-15},
	file = {BiGG Models ID Specification and Guidelines · SBRG/bigg_models Wiki:/Users/YuhouZhou/Zotero/storage/2HRSQLBB/BiGG-Models-ID-Specification-and-Guidelines.html:text/html}
}

@article{yahya_development_nodate,
	title = {Development of a {Scalable} {Distributed} {System} for {Image} {Classiﬁcation} {Task} in a {Hadoop} {Ecosystem}},
	language = {en},
	author = {Yahya, Sadok Ben},
	pages = {131},
	file = {Yahya - Development of a Scalable Distributed System for I.pdf:/Users/YuhouZhou/Zotero/storage/JE5AG3AX/Yahya - Development of a Scalable Distributed System for I.pdf:application/pdf}
}

@article{king_bigg_2016,
	title = {{BiGG} {Models}: {A} platform for integrating, standardizing and sharing genome-scale models},
	volume = {44},
	issn = {0305-1048, 1362-4962},
	shorttitle = {{BiGG} {Models}},
	url = {https://academic.oup.com/nar/article-lookup/doi/10.1093/nar/gkv1049},
	doi = {10.1093/nar/gkv1049},
	abstract = {Genome-scale metabolic models are mathematicallystructured knowledge bases that can be used to predict metabolic pathway usage and growth phenotypes. Furthermore, they can generate and test hypotheses when integrated with experimental data. To maximize the value of these models, centralized repositories of high-quality models must be established, models must adhere to established standards and model components must be linked to relevant databases. Tools for model visualization further enhance their utility. To meet these needs, we present BiGG Models (http://bigg.ucsd. edu), a completely redesigned Biochemical, Genetic and Genomic knowledge base. BiGG Models contains more than 75 high-quality, manually-curated genome-scale metabolic models. On the website, users can browse, search and visualize models. BiGG Models connects genome-scale models to genome annotations and external databases. Reaction and metabolite identiﬁers have been standardized across models to conform to community standards and enable rapid comparison across models. Furthermore, BiGG Models provides a comprehensive application programming interface for accessing BiGG Models with modeling and analysis tools. As a resource for highly curated, standardized and accessible models of metabolism, BiGG Models will facilitate diverse systems biology studies and support knowledge-based analysis of diverse experimental data.},
	language = {en},
	number = {D1},
	urldate = {2020-02-09},
	journal = {Nucleic Acids Research},
	author = {King, Zachary A. and Lu, Justin and Dräger, Andreas and Miller, Philip and Federowicz, Stephen and Lerman, Joshua A. and Ebrahim, Ali and Palsson, Bernhard O. and Lewis, Nathan E.},
	month = jan,
	year = {2016},
	pages = {D515--D522},
	file = {King et al. - 2016 - BiGG Models A platform for integrating, standardi.pdf:/Users/YuhouZhou/Zotero/storage/ZPR47LHN/King et al. - 2016 - BiGG Models A platform for integrating, standardi.pdf:application/pdf}
}

@misc{noauthor_geo_nodate,
	title = {{GEO} {Accession} viewer},
	url = {https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE130549},
	urldate = {2020-02-10},
	file = {GEO Accession viewer:/Users/YuhouZhou/Zotero/storage/XQYFI47F/acc.html:text/html}
}

@misc{noauthor_geo_nodate-1,
	title = {{GEO} {Accession} viewer},
	url = {https://www.ncbi.nlm.nih.gov/geo/query/acc.cgi?acc=GSE17257},
	urldate = {2020-02-10}
}